{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from detectron2.modeling import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from detectron2.config import get_cfg\n",
    "from config import get_settings\n",
    "from ai.ditod import add_vit_config\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    dataset = \"doclaynet\"\n",
    "    config_file_path = \"ai/Configs/doclaynet_VGT_cascade_PTM.yaml\"\n",
    "    model_weights_path = \"./models/model_final.pth\"\n",
    "\n",
    "\n",
    "args = Arguments()\n",
    "cfg = get_cfg()\n",
    "add_vit_config(cfg)\n",
    "cfg.merge_from_file(args.config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_pretrain_weight: load model from: ./models/layoutlm/\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import ImageList, Instances\n",
    "import torch\n",
    "def preprocess_image(images):\n",
    "    \"\"\"\n",
    "    Normalize, pad and batch the input images.\n",
    "    \"\"\"\n",
    "    pixel_mean, pixel_std = torch.tensor([103.530, 116.280, 123.675]), torch.tensor([57.375, 57.120, 58.395])\n",
    "    # images = [x[\"image\"].to('cpu') for x in batched_inputs]\n",
    "    images = [(x.permute(2,1,0) - pixel_mean) / pixel_std for x in images]\n",
    "    images = [x.permute(2,1,0) for x in images]\n",
    "    images = ImageList.from_tensors(\n",
    "        images,\n",
    "        0,\n",
    "        padding_constraints={},\n",
    "    )\n",
    "    print(images.tensor.shape)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "input_tensor = torch.rand(3,1000,1000)\n",
    "input_dict = [{\n",
    "    'image': input_tensor,\n",
    "    'input_ids':torch.tensor([2,444,566]),\n",
    "    'bbox': torch.tensor([[112,334,444,666], [23,35,666,435], [22,66,343,432]]),\n",
    "    'height': torch.tensor(1000),\n",
    "    'width': torch.tensor(1000)\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGT(\n",
       "  (backbone): GridFPN(\n",
       "    (fpn_lateral2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): PTM_VIT_Backbone(\n",
       "      (backbone): BEiT(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (grid_patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(64, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.00909090880304575)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0181818176060915)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.027272727340459824)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.036363635212183)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.045454543083906174)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.054545458406209946)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.06363636255264282)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0727272778749466)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.08181818574666977)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.09090909361839294)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.10000000149011612)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (grid_blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.00909090880304575)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0181818176060915)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.027272727340459824)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.036363635212183)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.045454543083906174)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.054545458406209946)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.06363636255264282)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0727272778749466)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.08181818574666977)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.09090909361839294)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.10000000149011612)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cross_blocks): ModuleList()\n",
       "        (fpn1): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (fpn2): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (fpn3): Identity()\n",
       "        (fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (grid_fpn1): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (grid_fpn2): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (grid_fpn3): Identity()\n",
       "        (grid_fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (FeatureMerge): FeatureMerge(\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): CascadeROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): ModuleList(\n",
       "      (0-2): 3 x FastRCNNConvFCHead(\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc_relu1): ReLU()\n",
       "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (fc_relu2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (box_predictor): ModuleList(\n",
       "      (0-2): 3 x FastRCNNOutputLayers(\n",
       "        (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Wordgrid_embedding): WordnnEmbedding(\n",
       "    (embedding): Embedding(30522, 768)\n",
       "    (embedding_proj): Linear(in_features=768, out_features=64, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/image_list.py:85: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert t.shape[:-2] == tensors[0].shape[:-2], t.shape\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/Wordnn_embedding.py:81: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  short_length_w = min(len(per_input_ids), len(per_input_bbox))\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/Wordnn_embedding.py:92: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  bbox.cpu().detach().numpy().round().astype(int).tolist()\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/VGTbeit.py:1156: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  training_window_size = torch.tensor([Hp, Wp])\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/VGTbeit.py:1157: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  grid_training_window_size = torch.tensor([grid_Hp, grid_Wp])\n"
     ]
    }
   ],
   "source": [
    "torch_script = torch.jit.trace(model.forward, (input_dict, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3068, 0.9300],\n",
      "        [0.0045, 0.7737]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3068, 0.9300],\n",
       "        [0.0045, 0.7737]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "quant =torch.ao.quantization.QuantStub()\n",
    "dequant = torch.ao.quantization.DeQuantStub()\n",
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "quant(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import OperatorExportTypes\n",
    "import onnx\n",
    "import io\n",
    "def export_onnx_model(model, inputs):\n",
    "    \"\"\"\n",
    "    Trace and export a model to onnx format.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module):\n",
    "        inputs (tuple[args]): the model will be called by `model(*inputs)`\n",
    "\n",
    "    Returns:\n",
    "        an onnx model\n",
    "    \"\"\"\n",
    "    assert isinstance(model, torch.nn.Module)\n",
    "\n",
    "    # make sure all modules are in eval mode, onnx may change the training state\n",
    "    # of the module if the states are not consistent\n",
    "    def _check_eval(module):\n",
    "        assert not module.training\n",
    "\n",
    "    model.apply(_check_eval)\n",
    "\n",
    "    # Export the model to ONNX\n",
    "    with torch.no_grad():\n",
    "        with io.BytesIO() as f:\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                inputs,\n",
    "                f,\n",
    "                operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,\n",
    "                verbose=True,  # NOTE: uncomment this for debugging\n",
    "                export_params=True,\n",
    "                input_names=['batched_inputs']\n",
    "            )\n",
    "            onnx_model = onnx.load_from_string(f.getvalue())\n",
    "            onnx.save(onnx_model, \"model.onnx\")\n",
    "\n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/image_list.py:85: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert t.shape[:-2] == tensors[0].shape[:-2], t.shape\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/Wordnn_embedding.py:81: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  short_length_w = min(len(per_input_ids), len(per_input_bbox))\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/Wordnn_embedding.py:92: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  bbox.cpu().detach().numpy().round().astype(int).tolist()\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/VGTbeit.py:1156: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  training_window_size = torch.tensor([Hp, Wp])\n",
      "/home/saeed/Walnut/Quantization/ai/ditod/VGTbeit.py:1157: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  grid_training_window_size = torch.tensor([grid_Hp, grid_Wp])\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:151: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor.numel() == 0:\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert tensor.dim() == 2 and tensor.size(-1) == 4, tensor.size()\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:151: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor.numel() == 0:\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert tensor.dim() == 2 and tensor.size(-1) == 4, tensor.size()\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/modeling/proposal_generator/proposal_utils.py:106: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not valid_mask.all():\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:192: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  h, w = box_size\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/layers/nms.py:15: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert boxes.shape[-1] == 4\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/__init__.py:1404: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/layers/roi_align.py:55: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert rois.dim() == 2 and rois.size(1) == 5\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:151: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor.numel() == 0:\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert tensor.dim() == 2 and tensor.size(-1) == 4, tensor.size()\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:192: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  h, w = box_size\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/__init__.py:1404: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/layers/roi_align.py:55: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert rois.dim() == 2 and rois.size(1) == 5\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:151: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor.numel() == 0:\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert tensor.dim() == 2 and tensor.size(-1) == 4, tensor.size()\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:192: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  h, w = box_size\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/__init__.py:1404: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/layers/roi_align.py:55: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert rois.dim() == 2 and rois.size(1) == 5\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/modeling/roi_heads/fast_rcnn.py:138: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not valid_mask.all():\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:151: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor.numel() == 0:\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert tensor.dim() == 2 and tensor.size(-1) == 4, tensor.size()\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:192: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  h, w = box_size\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/modeling/roi_heads/fast_rcnn.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_bbox_reg_classes == 1:\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/layers/nms.py:15: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert boxes.shape[-1] == 4\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:243: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for results_per_image, input_per_image, image_size in zip(\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:191: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:192: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  h, w = box_size\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:151: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if tensor.numel() == 0:\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/structures/boxes.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert tensor.dim() == 2 and tensor.size(-1) == 4, tensor.size()\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5856: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "onnx_model= export_onnx_model(model, (input_dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Simple module for demonstration\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n",
    "\n",
    "module = MyModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx as onnx\n",
    "onnx.export(module, args=(inp), f=\"aba.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "inference = onnxruntime.InferenceSession(\"/home/saeed/Walnut/Quantization/model.onnx\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized = torch.ao.quantization.quantize_dynamic(model ,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized(image_batch, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy(batched_inputs)\n",
      "Proxy(getattr_1)\n",
      "Proxy(size_1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (tuple, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# a tuple of one or more example inputs are needed to trace the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# example_inputs = (torch.rand(3,224,224))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# prepare\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m example_input \u001b[39m=\u001b[39m (image_batch, input_dict,)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model_prepared \u001b[39m=\u001b[39m quantize_fx\u001b[39m.\u001b[39;49mprepare_fx(model_to_quantize, qconfig_mapping, {\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_input})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# no calibration needed when we only have dynamic/weight_only quantization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# quantize\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model_quantized \u001b[39m=\u001b[39m quantize_fx\u001b[39m.\u001b[39mconvert_fx(model_prepared)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:382\u001b[0m, in \u001b[0;36mprepare_fx\u001b[0;34m(model, qconfig_mapping, example_inputs, prepare_custom_config, _equalization_config, backend_config)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" Prepare a model for post training static quantization\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    calibrate(prepared_model, sample_inference_data)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mquantization_api.quantize_fx.prepare_fx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m _prepare_fx(\n\u001b[1;32m    383\u001b[0m     model,\n\u001b[1;32m    384\u001b[0m     qconfig_mapping,\n\u001b[1;32m    385\u001b[0m     \u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# is_qat\u001b[39;49;00m\n\u001b[1;32m    386\u001b[0m     example_inputs,\n\u001b[1;32m    387\u001b[0m     prepare_custom_config,\n\u001b[1;32m    388\u001b[0m     _equalization_config,\n\u001b[1;32m    389\u001b[0m     backend_config,\n\u001b[1;32m    390\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:135\u001b[0m, in \u001b[0;36m_prepare_fx\u001b[0;34m(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, _equalization_config, backend_config, is_standalone_module)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m# symbolically trace the model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m tracer \u001b[39m=\u001b[39m QuantizationTracer(skipped_module_names, skipped_module_classes)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m graph_module \u001b[39m=\u001b[39m GraphModule(model, tracer\u001b[39m.\u001b[39;49mtrace(model))\n\u001b[1;32m    136\u001b[0m _attach_meta_to_node_if_not_exist(graph_module)\n\u001b[1;32m    138\u001b[0m fuse_custom_config \u001b[39m=\u001b[39m FuseCustomConfig()\u001b[39m.\u001b[39mset_preserved_attributes(prepare_custom_config\u001b[39m.\u001b[39mpreserved_attributes)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:101\u001b[0m, in \u001b[0;36mVGT.forward\u001b[0;34m(self, image_batch, batched_inputs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(image_batch, batched_inputs)\n\u001b[1;32m    102\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batched_inputs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:160\u001b[0m, in \u001b[0;36mVGT.inference\u001b[0;34m(self, image_batch, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    159\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)11\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m chargrid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWordgrid_embedding(image_batch\u001b[39m.\u001b[39;49mtensor, batched_inputs)\n\u001b[1;32m    161\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(image_batch\u001b[39m.\u001b[39mtensor, chargrid)\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m detected_instances \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(mod, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m, mod), \u001b[39m\"\u001b[39m\u001b[39m__globals__\u001b[39m\u001b[39m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_module(mod, forward, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_stack[_scope\u001b[39m.\u001b[39mmodule_path] \u001b[39m=\u001b[39m _scope\u001b[39m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[39m=\u001b[39m forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_proxy(\u001b[39m\"\u001b[39m\u001b[39mcall_module\u001b[39m\u001b[39m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/Wordnn_embedding.py:77\u001b[0m, in \u001b[0;36mWordnnEmbedding.forward\u001b[0;34m(self, img, batched_inputs, stride)\u001b[0m\n\u001b[1;32m     74\u001b[0m batch_b, _, batch_h, batch_w \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize()\n\u001b[1;32m     75\u001b[0m \u001b[39mprint\u001b[39m(img\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> 77\u001b[0m chargrid_map \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\n\u001b[1;32m     78\u001b[0m     (batch_b, batch_h \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m stride, batch_w \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m stride), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64\n\u001b[1;32m     79\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch_Size is >.......\u001b[39m\u001b[39m\"\u001b[39m, batch_b)\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m iter_b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_b):\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (tuple, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "\n",
    "\n",
    "#\n",
    "# post training dynamic/weight_only quantization\n",
    "#\n",
    "\n",
    "# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\n",
    "model_to_quantize = copy.deepcopy(model)\n",
    "model_to_quantize.eval()\n",
    "qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n",
    "# a tuple of one or more example inputs are needed to trace the model\n",
    "# example_inputs = (torch.rand(3,224,224))\n",
    "# prepare\n",
    "example_input = (image_batch, input_dict,)\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, {\"\": example_input})\n",
    "# no calibration needed when we only have dynamic/weight_only quantization\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(list(model.children())[0].children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/saeed/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:11<00:00, 14.1MB/s] \n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/functional.py:3964: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/ops/boxes.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/ops/boxes.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/__init__.py:1404: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/detection/transform.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/detection/transform.py:307: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5856: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dummpy_input = torch.randn(1, 3, 1333, 800)\n",
    "model.eval()\n",
    "model(dummpy_input)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  dummpy_input,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"model.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
