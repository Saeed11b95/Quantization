{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from detectron2.modeling import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from detectron2.config import get_cfg\n",
    "from config import get_settings\n",
    "from ai.ditod import add_vit_config\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    dataset = \"doclaynet\"\n",
    "    config_file_path = \"ai/Configs/doclaynet_VGT_cascade_PTM.yaml\"\n",
    "    model_weights_path = \"./models/model_final.pth\"\n",
    "    embedding_weights = \"/home/saeed/Walnut/Quantization/models/layoutlm/\"\n",
    "\n",
    "\n",
    "args = Arguments()\n",
    "cfg = get_cfg()\n",
    "add_vit_config(cfg)\n",
    "cfg.merge_from_file(args.config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "wrap must be called at the top level of a module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m build_model(cfg)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/modeling/meta_arch/build.py:22\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mBuild the whole model architecture, defined by ``cfg.MODEL.META_ARCHITECTURE``.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mNote that it does not load any weights from ``cfg``.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m meta_arch \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mMETA_ARCHITECTURE\n\u001b[0;32m---> 22\u001b[0m model \u001b[39m=\u001b[39m META_ARCH_REGISTRY\u001b[39m.\u001b[39;49mget(meta_arch)(cfg)\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mDEVICE))\n\u001b[1;32m     24\u001b[0m _log_api_usage(\u001b[39m\"\u001b[39m\u001b[39mmodeling.meta_arch.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m meta_arch)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/config/config.py:189\u001b[0m, in \u001b[0;36mconfigurable.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mClass with @configurable must have a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfrom_config\u001b[39m\u001b[39m'\u001b[39m\u001b[39m classmethod.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    188\u001b[0m \u001b[39mif\u001b[39;00m _called_with_cfg(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 189\u001b[0m     explicit_args \u001b[39m=\u001b[39m _get_args_from_config(from_config_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    190\u001b[0m     init_func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexplicit_args)\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/config/config.py:245\u001b[0m, in \u001b[0;36m_get_args_from_config\u001b[0;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m supported_arg_names:\n\u001b[1;32m    244\u001b[0m         extra_kwargs[name] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(name)\n\u001b[0;32m--> 245\u001b[0m ret \u001b[39m=\u001b[39m from_config_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    246\u001b[0m \u001b[39m# forward the other arguments to __init__\u001b[39;00m\n\u001b[1;32m    247\u001b[0m ret\u001b[39m.\u001b[39mupdate(extra_kwargs)\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:64\u001b[0m, in \u001b[0;36mVGT.from_config\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_config\u001b[39m(\u001b[39mcls\u001b[39m, cfg):\n\u001b[0;32m---> 64\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_config(cfg)\n\u001b[1;32m     65\u001b[0m     ret\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m     66\u001b[0m         {\n\u001b[1;32m     67\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m\"\u001b[39m: cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mWORDGRID\u001b[39m.\u001b[39mVOCAB_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         }\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:73\u001b[0m, in \u001b[0;36mGeneralizedRCNN.from_config\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_config\u001b[39m(\u001b[39mcls\u001b[39m, cfg):\n\u001b[0;32m---> 73\u001b[0m     backbone \u001b[39m=\u001b[39m build_backbone(cfg)\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     75\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbackbone\u001b[39m\u001b[39m\"\u001b[39m: backbone,\n\u001b[1;32m     76\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mproposal_generator\u001b[39m\u001b[39m\"\u001b[39m: build_proposal_generator(cfg, backbone\u001b[39m.\u001b[39moutput_shape()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpixel_std\u001b[39m\u001b[39m\"\u001b[39m: cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mPIXEL_STD,\n\u001b[1;32m     82\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/detectron2/modeling/backbone/build.py:31\u001b[0m, in \u001b[0;36mbuild_backbone\u001b[0;34m(cfg, input_shape)\u001b[0m\n\u001b[1;32m     28\u001b[0m     input_shape \u001b[39m=\u001b[39m ShapeSpec(channels\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mPIXEL_MEAN))\n\u001b[1;32m     30\u001b[0m backbone_name \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mBACKBONE\u001b[39m.\u001b[39mNAME\n\u001b[0;32m---> 31\u001b[0m backbone \u001b[39m=\u001b[39m BACKBONE_REGISTRY\u001b[39m.\u001b[39;49mget(backbone_name)(cfg, input_shape)\n\u001b[1;32m     32\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(backbone, Backbone)\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m backbone\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbackbone.py:280\u001b[0m, in \u001b[0;36mbuild_VGT_fpn_backbone\u001b[0;34m(cfg, input_shape)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m@BACKBONE_REGISTRY\u001b[39m\u001b[39m.\u001b[39mregister()\n\u001b[1;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_VGT_fpn_backbone\u001b[39m(cfg, input_shape: ShapeSpec):\n\u001b[1;32m    271\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m    Create a VIT w/ FPN backbone.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39m        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     bottom_up \u001b[39m=\u001b[39m build_PTM_VIT_Backbone(cfg)\n\u001b[1;32m    281\u001b[0m     in_features \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mFPN\u001b[39m.\u001b[39mIN_FEATURES\n\u001b[1;32m    282\u001b[0m     out_channels \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mFPN\u001b[39m.\u001b[39mOUT_CHANNELS\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbackbone.py:266\u001b[0m, in \u001b[0;36mbuild_PTM_VIT_Backbone\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    262\u001b[0m merge_type \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mVIT\u001b[39m.\u001b[39mMERGE_TYPE\n\u001b[1;32m    264\u001b[0m model_kwargs \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(\u001b[39mstr\u001b[39m(cfg\u001b[39m.\u001b[39mMODEL\u001b[39m.\u001b[39mVIT\u001b[39m.\u001b[39mMODEL_KWARGS)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 266\u001b[0m \u001b[39mreturn\u001b[39;00m PTM_VIT_Backbone(name, out_features, drop_path, img_size, pos_type, merge_type, model_kwargs)\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbackbone.py:130\u001b[0m, in \u001b[0;36mPTM_VIT_Backbone.__init__\u001b[0;34m(self, name, out_features, drop_path, img_size, pos_type, merge_type, model_kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mbeit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name:\n\u001b[1;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m pos_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 130\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone \u001b[39m=\u001b[39m model_func(\n\u001b[1;32m    131\u001b[0m             img_size\u001b[39m=\u001b[39;49mimg_size,\n\u001b[1;32m    132\u001b[0m             out_features\u001b[39m=\u001b[39;49mout_features,\n\u001b[1;32m    133\u001b[0m             drop_path_rate\u001b[39m=\u001b[39;49mdrop_path,\n\u001b[1;32m    134\u001b[0m             use_abs_pos_emb\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    135\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m     \u001b[39melif\u001b[39;00m pos_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mshared_rel\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone \u001b[39m=\u001b[39m model_func(\n\u001b[1;32m    139\u001b[0m             img_size\u001b[39m=\u001b[39mimg_size,\n\u001b[1;32m    140\u001b[0m             out_features\u001b[39m=\u001b[39mout_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    144\u001b[0m         )\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbeit.py:1267\u001b[0m, in \u001b[0;36mVGT_dit_base_patch16\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mVGT_dit_base_patch16\u001b[39m(pretrained\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1267\u001b[0m     model \u001b[39m=\u001b[39m BEiT(\n\u001b[1;32m   1268\u001b[0m         patch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m   1269\u001b[0m         embed_dim\u001b[39m=\u001b[39;49m\u001b[39m768\u001b[39;49m,\n\u001b[1;32m   1270\u001b[0m         self_depth\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[1;32m   1271\u001b[0m         cross_depth\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   1272\u001b[0m         num_heads\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[1;32m   1273\u001b[0m         mlp_ratio\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m   1274\u001b[0m         qkv_bias\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1275\u001b[0m         norm_layer\u001b[39m=\u001b[39;49mpartial(nn\u001b[39m.\u001b[39;49mLayerNorm, eps\u001b[39m=\u001b[39;49m\u001b[39m1e-6\u001b[39;49m),\n\u001b[1;32m   1276\u001b[0m         init_values\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m   1277\u001b[0m         in_chans\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m   1278\u001b[0m         grid_chans\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[1;32m   1279\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1280\u001b[0m     )\n\u001b[1;32m   1281\u001b[0m     model\u001b[39m.\u001b[39mdefault_cfg \u001b[39m=\u001b[39m _cfg()\n\u001b[1;32m   1282\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbeit.py:893\u001b[0m, in \u001b[0;36mBEiT.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, grid_chans, num_classes, embed_dim, self_depth, cross_depth, num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, drop_path_rate, hybrid_backbone, norm_layer, init_values, use_abs_pos_emb, use_rel_pos_bias, use_shared_rel_pos_bias, use_checkpoint, pretrained, out_features)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features \u001b[39m=\u001b[39m (\n\u001b[1;32m    890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim\n\u001b[1;32m    891\u001b[0m ) \u001b[39m=\u001b[39m embed_dim  \u001b[39m# num_features for consistency with other models\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_checkpoint \u001b[39m=\u001b[39m use_checkpoint\n\u001b[0;32m--> 893\u001b[0m torch\u001b[39m.\u001b[39;49mfx\u001b[39m.\u001b[39;49mwrap(\u001b[39m\"\u001b[39;49m\u001b[39mlen\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m hybrid_backbone \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embed \u001b[39m=\u001b[39m HybridEmbed(\n\u001b[1;32m    896\u001b[0m         hybrid_backbone,\n\u001b[1;32m    897\u001b[0m         img_size\u001b[39m=\u001b[39mimg_size,\n\u001b[1;32m    898\u001b[0m         in_chans\u001b[39m=\u001b[39min_chans,\n\u001b[1;32m    899\u001b[0m         embed_dim\u001b[39m=\u001b[39membed_dim,\n\u001b[1;32m    900\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1089\u001b[0m, in \u001b[0;36mwrap\u001b[0;34m(fn_or_name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[39massert\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<module>\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mwrap must be called at the top level of a module\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1091\u001b[0m \u001b[39m# consider implementing Callable version of this via _autowrap_function_ids / _autowrap_search\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39m# semantics would be slightly different, but would add support `from x import wrapped_function`\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m _wrapped_fns_to_patch[(\u001b[39mid\u001b[39m(f\u001b[39m.\u001b[39mf_globals), fn_name)] \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mf_globals\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: wrap must be called at the top level of a module"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from ai.ditod.tokenization_bros import BrosTokenizer\n",
    "\n",
    "\n",
    "def _init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # we use xavier_uniform following official JAX ViT:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "class WordnnEmbedding(nn.Module):\n",
    "    \"\"\"Generate chargrid embedding feature map.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30552,\n",
    "        hidden_size=768,\n",
    "        embedding_dim=64,\n",
    "        bros_embedding_path=\"/bros-base-uncased/\",\n",
    "        use_pretrain_weight=True,\n",
    "        use_UNK_text=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args：\n",
    "            vocab_size (int): size of vocabulary.\n",
    "            embedding_dim (int): dim of input features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.use_pretrain_weight = use_pretrain_weight\n",
    "        self.use_UNK_text = use_UNK_text\n",
    "\n",
    "        self.init_weights(bros_embedding_path)\n",
    "        self.apply(_init_weights)\n",
    "\n",
    "    def init_weights(self, bros_embedding_path):\n",
    "        if self.use_pretrain_weight:\n",
    "            state_dict = torch.load(\n",
    "                bros_embedding_path + \"pytorch_model.bin\", map_location=\"cpu\"\n",
    "            )\n",
    "            if \"bert\" in bros_embedding_path:\n",
    "                word_embs = state_dict[\"bert.embeddings.word_embeddings.weight\"]\n",
    "            elif \"bros\" in bros_embedding_path:\n",
    "                word_embs = state_dict[\"embeddings.word_embeddings.weight\"]\n",
    "            elif \"layoutlm\" in bros_embedding_path:\n",
    "                word_embs = state_dict[\"layoutlm.embeddings.word_embeddings.weight\"]\n",
    "            else:\n",
    "                print(\"Wrong bros_embedding_path!\")\n",
    "            self.embedding = nn.Embedding.from_pretrained(word_embs)\n",
    "            print(\"use_pretrain_weight: load model from:\", bros_embedding_path)\n",
    "\n",
    "    def forward(self, img, batched_inputs, stride=1):\n",
    "        \"\"\"Forward computation\n",
    "        Args:\n",
    "            img (Tensor): in shape of [B x 3 x H x W]\n",
    "            batched_inputs (list[dict]):\n",
    "        Returns:\n",
    "            Tensor: in shape of [B x N x L x D], where D is the embedding_dim.\n",
    "        \"\"\"\n",
    "        device = img.device\n",
    "        batch_b, _, batch_h, batch_w = img.size()\n",
    "\n",
    "        chargrid_map = torch.zeros(\n",
    "            (batch_b, batch_h // stride, batch_w // stride), dtype=torch.int64\n",
    "        ).to(device)\n",
    "        for iter_b in range(batch_b):\n",
    "            per_input_ids = batched_inputs[iter_b][\"input_ids\"]\n",
    "            per_input_bbox = batched_inputs[iter_b][\"bbox\"]\n",
    "\n",
    "            short_length_w = min(len(per_input_ids), len(per_input_bbox))\n",
    "\n",
    "            if short_length_w > 0:\n",
    "                for word_idx in range(short_length_w):\n",
    "                    per_id = per_input_ids[word_idx]\n",
    "\n",
    "                    bbox = per_input_bbox[word_idx] / stride\n",
    "                    # w_start, h_start, w_end, h_end = (\n",
    "                    #     bbox.round().astype(np.int).tolist()\n",
    "                    # )\n",
    "                    w_start, h_start, w_end, h_end = (\n",
    "                        bbox.cpu().detach().numpy().round().astype(int).tolist()\n",
    "                    )\n",
    "                    if self.use_UNK_text:\n",
    "                        chargrid_map[iter_b, h_start:h_end, w_start:w_end] = 100\n",
    "                    else:\n",
    "                        chargrid_map[iter_b, h_start:h_end, w_start:w_end] = per_id\n",
    "        chargrid_map = self.embedding(chargrid_map)\n",
    "        return chargrid_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_pretrain_weight: load model from: /home/saeed/Walnut/Quantization/models/layoutlm/\n"
     ]
    }
   ],
   "source": [
    "gird_embedding = WordnnEmbedding(bros_embedding_path=args.embedding_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import ImageList\n",
    "import torch\n",
    "def preprocess_image(images):\n",
    "    \"\"\"\n",
    "    Normalize, pad and batch the input images.\n",
    "    \"\"\"\n",
    "    pixel_mean, pixel_std = torch.tensor([103.530, 116.280, 123.675]), torch.tensor([57.375, 57.120, 58.395])\n",
    "    images = [(x.permute(2,1,0) - pixel_mean) / pixel_std for x in images]\n",
    "    images = [x.permute(2,1,0) for x in images]\n",
    "    images = ImageList.from_tensors(\n",
    "        images,\n",
    "        0,\n",
    "        padding_constraints={},\n",
    "    )\n",
    "    print(images.tensor.shape)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "input_tensor = torch.rand(3,1000,1000)\n",
    "sizes_dict =     [{'height': torch.tensor(1000),'width': torch.tensor(1000)}]\n",
    "input_dict = [{\n",
    "    'input_ids':torch.tensor([2,444,566]),\n",
    "    'bbox': torch.tensor([[112,334,444,666], [23,35,666,435], [22,66,343,432]]),\n",
    "}]\n",
    "imagelist = preprocess_image([input_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gird_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grid \u001b[39m=\u001b[39m gird_embedding(imagelist\u001b[39m.\u001b[39mtensor, input_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gird_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "grid = gird_embedding(imagelist.tensor, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGT(\n",
       "  (backbone): GridFPN(\n",
       "    (fpn_lateral2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): PTM_VIT_Backbone(\n",
       "      (backbone): BEiT(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (grid_patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(64, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.00909090880304575)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0181818176060915)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.027272727340459824)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.036363635212183)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.045454543083906174)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.054545458406209946)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.06363636255264282)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0727272778749466)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.08181818574666977)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.09090909361839294)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.10000000149011612)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (grid_blocks): ModuleList(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.00909090880304575)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0181818176060915)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.027272727340459824)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.036363635212183)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.045454543083906174)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.054545458406209946)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.06363636255264282)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.0727272778749466)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.08181818574666977)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.09090909361839294)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): DropPath(p=0.10000000149011612)\n",
       "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cross_blocks): ModuleList()\n",
       "        (fpn1): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (fpn2): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (fpn3): Identity()\n",
       "        (fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (grid_fpn1): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (grid_fpn2): Sequential(\n",
       "          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (grid_fpn3): Identity()\n",
       "        (grid_fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (FeatureMerge): FeatureMerge(\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): CascadeROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): ModuleList(\n",
       "      (0-2): 3 x FastRCNNConvFCHead(\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc_relu1): ReLU()\n",
       "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (fc_relu2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (box_predictor): ModuleList(\n",
       "      (0-2): 3 x FastRCNNOutputLayers(\n",
       "        (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding_proj): Linear(in_features=768, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_script = torch.jit.trace(model.forward, (imagelist, input_dict ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program = torch.onnx.dynamo_export(model, imagelist, grid, sizes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "quant =torch.ao.quantization.QuantStub()\n",
    "dequant = torch.ao.quantization.DeQuantStub()\n",
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "quant(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import OperatorExportTypes\n",
    "import onnx\n",
    "import io\n",
    "def export_onnx_model(model, inputs):\n",
    "    \"\"\"\n",
    "    Trace and export a model to onnx format.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module):\n",
    "        inputs (tuple[args]): the model will be called by `model(*inputs)`\n",
    "\n",
    "    Returns:\n",
    "        an onnx model\n",
    "    \"\"\"\n",
    "    assert isinstance(model, torch.nn.Module)\n",
    "\n",
    "    # make sure all modules are in eval mode, onnx may change the training state\n",
    "    # of the module if the states are not consistent\n",
    "    def _check_eval(module):\n",
    "        assert not module.training\n",
    "\n",
    "    model.apply(_check_eval)\n",
    "\n",
    "    # Export the model to ONNX\n",
    "    with torch.no_grad():\n",
    "        with io.BytesIO() as f:\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                inputs,\n",
    "                f,\n",
    "                operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,\n",
    "                # verbose=True,  # NOTE: uncomment this for debugging\n",
    "                # export_params=True,\n",
    "                input_names=['batched_inputs']\n",
    "            )\n",
    "            onnx_model = onnx.load_from_string(f.getvalue())\n",
    "            onnx.save(onnx_model, \"model.onnx\")\n",
    "\n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model= export_onnx_model(model, (input_dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "session = onnxruntime.InferenceSession(\"/home/saeed/Walnut/Quantization/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Simple module for demonstration\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n",
    "\n",
    "module = MyModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx as onnx\n",
    "onnx.export(module, args=(inp), f=\"aba.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "inference = onnxruntime.InferenceSession(\"/home/saeed/Walnut/Quantization/model.onnx\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized = torch.ao.quantization.quantize_dynamic(model ,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized(image_batch, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# a tuple of one or more example inputs are needed to trace the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# example_inputs = (torch.rand(3,224,224))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# prepare\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m example_input \u001b[39m=\u001b[39m (imagelist, grid, sizes_dict)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model_prepared \u001b[39m=\u001b[39m quantize_fx\u001b[39m.\u001b[39;49mprepare_fx(model_to_quantize, qconfig_mapping, {\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_input})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# no calibration needed when we only have dynamic/weight_only quantization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# quantize\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model_quantized \u001b[39m=\u001b[39m quantize_fx\u001b[39m.\u001b[39mconvert_fx(model_prepared)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:382\u001b[0m, in \u001b[0;36mprepare_fx\u001b[0;34m(model, qconfig_mapping, example_inputs, prepare_custom_config, _equalization_config, backend_config)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" Prepare a model for post training static quantization\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    calibrate(prepared_model, sample_inference_data)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mquantization_api.quantize_fx.prepare_fx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m _prepare_fx(\n\u001b[1;32m    383\u001b[0m     model,\n\u001b[1;32m    384\u001b[0m     qconfig_mapping,\n\u001b[1;32m    385\u001b[0m     \u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# is_qat\u001b[39;49;00m\n\u001b[1;32m    386\u001b[0m     example_inputs,\n\u001b[1;32m    387\u001b[0m     prepare_custom_config,\n\u001b[1;32m    388\u001b[0m     _equalization_config,\n\u001b[1;32m    389\u001b[0m     backend_config,\n\u001b[1;32m    390\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:135\u001b[0m, in \u001b[0;36m_prepare_fx\u001b[0;34m(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, _equalization_config, backend_config, is_standalone_module)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m# symbolically trace the model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m tracer \u001b[39m=\u001b[39m QuantizationTracer(skipped_module_names, skipped_module_classes)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m graph_module \u001b[39m=\u001b[39m GraphModule(model, tracer\u001b[39m.\u001b[39;49mtrace(model))\n\u001b[1;32m    136\u001b[0m _attach_meta_to_node_if_not_exist(graph_module)\n\u001b[1;32m    138\u001b[0m fuse_custom_config \u001b[39m=\u001b[39m FuseCustomConfig()\u001b[39m.\u001b[39mset_preserved_attributes(prepare_custom_config\u001b[39m.\u001b[39mpreserved_attributes)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:101\u001b[0m, in \u001b[0;36mVGT.forward\u001b[0;34m(self, images, grid, image_sizes, instances)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# if not self.training:\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(images, grid, image_sizes)\n\u001b[1;32m    102\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m gt_instances \u001b[39m=\u001b[39m instances\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:146\u001b[0m, in \u001b[0;36mVGT.inference\u001b[0;34m(self, images, grid, image_sizes, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m chargrid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_proj(grid)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m--> 146\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensor, chargrid)\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m detected_instances \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_generator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(mod, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m, mod), \u001b[39m\"\u001b[39m\u001b[39m__globals__\u001b[39m\u001b[39m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_module(mod, forward, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_stack[_scope\u001b[39m.\u001b[39mmodule_path] \u001b[39m=\u001b[39m _scope\u001b[39m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[39m=\u001b[39m forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_proxy(\u001b[39m\"\u001b[39m\u001b[39mcall_module\u001b[39m\u001b[39m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbackbone.py:211\u001b[0m, in \u001b[0;36mGridFPN.forward\u001b[0;34m(self, x, grid)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, grid):\n\u001b[1;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m        input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     bottom_up_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbottom_up(x, grid)\n\u001b[1;32m    212\u001b[0m     results \u001b[39m=\u001b[39m []\n\u001b[1;32m    213\u001b[0m     prev_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlateral_convs[\u001b[39m0\u001b[39m](bottom_up_features[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(mod, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m, mod), \u001b[39m\"\u001b[39m\u001b[39m__globals__\u001b[39m\u001b[39m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_module(mod, forward, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_stack[_scope\u001b[39m.\u001b[39mmodule_path] \u001b[39m=\u001b[39m _scope\u001b[39m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[39m=\u001b[39m forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_proxy(\u001b[39m\"\u001b[39m\u001b[39mcall_module\u001b[39m\u001b[39m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbackbone.py:184\u001b[0m, in \u001b[0;36mPTM_VIT_Backbone.forward\u001b[0;34m(self, x, grid)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39m    dict[str->Tensor]: names and the corresponding features\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m# assert (\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m#     x.dim() == 4\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# ), f\"VIT takes an input of shape (N, C, H, W). Got {x.shape} instead!\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m vis_feat_out, grid_feat_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone\u001b[39m.\u001b[39;49mforward_features(x, grid)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFeatureMerge\u001b[39m.\u001b[39mforward(vis_feat_out, grid_feat_out)\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbeit.py:1156\u001b[0m, in \u001b[0;36mBEiT.forward_features\u001b[0;34m(self, x, grid)\u001b[0m\n\u001b[1;32m   1154\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[1;32m   1155\u001b[0m grid_features \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1156\u001b[0m training_window_size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([Hp, Wp])\n\u001b[1;32m   1157\u001b[0m grid_training_window_size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([grid_Hp, grid_Wp])\n\u001b[1;32m   1159\u001b[0m rel_pos_bias \u001b[39m=\u001b[39m (\n\u001b[1;32m   1160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrel_pos_bias(training_window_size)\n\u001b[1;32m   1161\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrel_pos_bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/proxy.py:444\u001b[0m, in \u001b[0;36mProxy.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlen\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported in symbolic tracing by default. If you want \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mthis call to be recorded, please call torch.fx.wrap(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlen\u001b[39m\u001b[39m'\u001b[39m\u001b[39m) at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mmodule scope\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "\n",
    "\n",
    "#\n",
    "# post training dynamic/weight_only quantization\n",
    "#\n",
    "\n",
    "# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\n",
    "model_to_quantize = copy.deepcopy(model)\n",
    "model_to_quantize.eval()\n",
    "qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n",
    "# a tuple of one or more example inputs are needed to trace the model\n",
    "# example_inputs = (torch.rand(3,224,224))\n",
    "# prepare\n",
    "example_input = (imagelist, grid, sizes_dict)\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, {\"\": example_input})\n",
    "# no calibration needed when we only have dynamic/weight_only quantization\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(list(model.children())[0].children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "weights = torch.load(\"/home/saeed/Walnut/Quantization/models/model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(weights['model'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backbone.fpn_lateral2.weight',\n",
       " 'backbone.fpn_lateral2.bias',\n",
       " 'backbone.fpn_output2.weight',\n",
       " 'backbone.fpn_output2.bias',\n",
       " 'backbone.fpn_lateral3.weight',\n",
       " 'backbone.fpn_lateral3.bias',\n",
       " 'backbone.fpn_output3.weight',\n",
       " 'backbone.fpn_output3.bias',\n",
       " 'backbone.fpn_lateral4.weight',\n",
       " 'backbone.fpn_lateral4.bias',\n",
       " 'backbone.fpn_output4.weight',\n",
       " 'backbone.fpn_output4.bias',\n",
       " 'backbone.fpn_lateral5.weight',\n",
       " 'backbone.fpn_lateral5.bias',\n",
       " 'backbone.fpn_output5.weight',\n",
       " 'backbone.fpn_output5.bias',\n",
       " 'backbone.bottom_up.backbone.cls_token',\n",
       " 'backbone.bottom_up.backbone.grid_token',\n",
       " 'backbone.bottom_up.backbone.pos_embed',\n",
       " 'backbone.bottom_up.backbone.grid_pos_embed',\n",
       " 'backbone.bottom_up.backbone.patch_embed.proj.weight',\n",
       " 'backbone.bottom_up.backbone.patch_embed.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_patch_embed.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_patch_embed.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.0.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.0.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.0.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.0.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.0.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.0.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.0.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.0.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.0.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.0.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.0.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.0.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.0.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.0.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.0.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.1.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.1.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.1.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.1.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.1.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.1.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.1.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.1.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.1.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.1.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.1.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.1.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.1.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.1.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.1.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.2.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.2.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.2.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.2.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.2.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.2.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.2.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.2.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.2.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.2.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.2.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.2.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.2.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.2.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.2.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.3.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.3.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.3.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.3.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.3.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.3.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.3.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.3.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.3.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.3.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.3.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.3.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.3.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.3.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.3.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.4.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.4.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.4.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.4.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.4.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.4.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.4.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.4.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.4.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.4.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.4.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.4.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.4.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.4.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.4.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.5.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.5.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.5.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.5.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.5.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.5.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.5.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.5.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.5.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.5.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.5.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.5.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.5.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.5.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.5.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.6.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.6.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.6.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.6.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.6.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.6.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.6.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.6.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.6.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.6.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.6.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.6.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.6.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.6.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.6.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.7.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.7.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.7.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.7.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.7.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.7.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.7.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.7.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.7.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.7.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.7.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.7.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.7.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.7.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.7.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.8.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.8.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.8.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.8.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.8.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.8.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.8.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.8.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.8.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.8.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.8.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.8.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.8.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.8.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.8.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.9.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.9.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.9.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.9.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.9.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.9.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.9.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.9.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.9.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.9.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.9.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.9.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.9.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.9.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.9.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.10.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.10.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.10.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.10.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.10.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.10.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.10.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.10.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.10.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.10.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.10.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.10.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.10.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.10.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.10.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.11.gamma_1',\n",
       " 'backbone.bottom_up.backbone.blocks.11.gamma_2',\n",
       " 'backbone.bottom_up.backbone.blocks.11.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.11.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.11.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.11.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.blocks.11.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.11.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.11.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.11.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.11.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.11.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.11.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.blocks.11.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.blocks.11.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.0.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.1.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.2.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.3.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.4.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.5.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.6.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.7.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.8.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.9.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.10.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.gamma_1',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.gamma_2',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.norm1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.norm1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.attn.q_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.attn.v_bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.attn.qkv.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.attn.proj.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.attn.proj.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.norm2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.norm2.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.mlp.fc1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.mlp.fc1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.mlp.fc2.weight',\n",
       " 'backbone.bottom_up.backbone.grid_blocks.11.mlp.fc2.bias',\n",
       " 'backbone.bottom_up.backbone.fpn1.0.weight',\n",
       " 'backbone.bottom_up.backbone.fpn1.0.bias',\n",
       " 'backbone.bottom_up.backbone.fpn1.1.weight',\n",
       " 'backbone.bottom_up.backbone.fpn1.1.bias',\n",
       " 'backbone.bottom_up.backbone.fpn1.1.running_mean',\n",
       " 'backbone.bottom_up.backbone.fpn1.1.running_var',\n",
       " 'backbone.bottom_up.backbone.fpn1.1.num_batches_tracked',\n",
       " 'backbone.bottom_up.backbone.fpn1.3.weight',\n",
       " 'backbone.bottom_up.backbone.fpn1.3.bias',\n",
       " 'backbone.bottom_up.backbone.fpn2.0.weight',\n",
       " 'backbone.bottom_up.backbone.fpn2.0.bias',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.0.weight',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.0.bias',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.1.weight',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.1.bias',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.1.running_mean',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.1.running_var',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.1.num_batches_tracked',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.3.weight',\n",
       " 'backbone.bottom_up.backbone.grid_fpn1.3.bias',\n",
       " 'backbone.bottom_up.backbone.grid_fpn2.0.weight',\n",
       " 'backbone.bottom_up.backbone.grid_fpn2.0.bias',\n",
       " 'proposal_generator.rpn_head.conv.weight',\n",
       " 'proposal_generator.rpn_head.conv.bias',\n",
       " 'proposal_generator.rpn_head.objectness_logits.weight',\n",
       " 'proposal_generator.rpn_head.objectness_logits.bias',\n",
       " 'proposal_generator.rpn_head.anchor_deltas.weight',\n",
       " 'proposal_generator.rpn_head.anchor_deltas.bias',\n",
       " 'roi_heads.box_head.0.fc1.weight',\n",
       " 'roi_heads.box_head.0.fc1.bias',\n",
       " 'roi_heads.box_head.0.fc2.weight',\n",
       " 'roi_heads.box_head.0.fc2.bias',\n",
       " 'roi_heads.box_head.1.fc1.weight',\n",
       " 'roi_heads.box_head.1.fc1.bias',\n",
       " 'roi_heads.box_head.1.fc2.weight',\n",
       " 'roi_heads.box_head.1.fc2.bias',\n",
       " 'roi_heads.box_head.2.fc1.weight',\n",
       " 'roi_heads.box_head.2.fc1.bias',\n",
       " 'roi_heads.box_head.2.fc2.weight',\n",
       " 'roi_heads.box_head.2.fc2.bias',\n",
       " 'roi_heads.box_predictor.0.cls_score.weight',\n",
       " 'roi_heads.box_predictor.0.cls_score.bias',\n",
       " 'roi_heads.box_predictor.0.bbox_pred.weight',\n",
       " 'roi_heads.box_predictor.0.bbox_pred.bias',\n",
       " 'roi_heads.box_predictor.1.cls_score.weight',\n",
       " 'roi_heads.box_predictor.1.cls_score.bias',\n",
       " 'roi_heads.box_predictor.1.bbox_pred.weight',\n",
       " 'roi_heads.box_predictor.1.bbox_pred.bias',\n",
       " 'roi_heads.box_predictor.2.cls_score.weight',\n",
       " 'roi_heads.box_predictor.2.cls_score.bias',\n",
       " 'roi_heads.box_predictor.2.bbox_pred.weight',\n",
       " 'roi_heads.box_predictor.2.bbox_pred.bias',\n",
       " 'Wordgrid_embedding.embedding.weight',\n",
       " 'Wordgrid_embedding.embedding_proj.weight']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [k for k in x if 'embedding' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wordgrid_embedding.embedding.weight',\n",
       " 'Wordgrid_embedding.embedding_proj.weight']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
