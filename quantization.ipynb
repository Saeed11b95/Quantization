{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from detectron2.modeling import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from detectron2.config import get_cfg\n",
    "from config import get_settings\n",
    "from ai.ditod import add_vit_config\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    dataset = \"doclaynet\"\n",
    "    config_file_path = \"ai/Configs/doclaynet_VGT_cascade_PTM.yaml\"\n",
    "    model_weights_path = \"./models/model_final.pth\"\n",
    "\n",
    "\n",
    "args = Arguments()\n",
    "cfg = get_cfg()\n",
    "add_vit_config(cfg)\n",
    "cfg.merge_from_file(args.config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_pretrain_weight: load model from: ./models/layoutlm/\n"
     ]
    }
   ],
   "source": [
    "model = build_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import ImageList, Instances\n",
    "import torch\n",
    "def preprocess_image(images):\n",
    "    \"\"\"\n",
    "    Normalize, pad and batch the input images.\n",
    "    \"\"\"\n",
    "    pixel_mean, pixel_std = torch.tensor([103.530, 116.280, 123.675]), torch.tensor([57.375, 57.120, 58.395])\n",
    "    # images = [x[\"image\"].to('cpu') for x in batched_inputs]\n",
    "    images = [(x.permute(2,1,0) - pixel_mean) / pixel_std for x in images]\n",
    "    images = [x.permute(2,1,0) for x in images]\n",
    "    images = ImageList.from_tensors(\n",
    "        images,\n",
    "        0,\n",
    "        padding_constraints={},\n",
    "    )\n",
    "    print(images.tensor.shape)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "input_tensor = torch.rand(3,1000,1000)\n",
    "input_dict = [{\n",
    "    'input_ids':[2,444,566],\n",
    "    'bbox': np.array([[112,334,444,666], [23,35,666,435], [22,66,343,432]]),\n",
    "    'height': 1000,\n",
    "    'width': 1000\n",
    "}]\n",
    "image_batch= preprocess_image([input_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3068, 0.9300],\n",
      "        [0.0045, 0.7737]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3068, 0.9300],\n",
       "        [0.0045, 0.7737]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "quant =torch.ao.quantization.QuantStub()\n",
    "dequant = torch.ao.quantization.DeQuantStub()\n",
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "quant(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m onnx_program \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mdynamo_export(model, (image_batch, input_dict))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "onnx_program = torch.onnx.dynamo_export(model, (image_batch, input_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import OperatorExportTypes\n",
    "import onnx\n",
    "import io\n",
    "def export_onnx_model(model, inputs):\n",
    "    \"\"\"\n",
    "    Trace and export a model to onnx format.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module):\n",
    "        inputs (tuple[args]): the model will be called by `model(*inputs)`\n",
    "\n",
    "    Returns:\n",
    "        an onnx model\n",
    "    \"\"\"\n",
    "    assert isinstance(model, torch.nn.Module)\n",
    "\n",
    "    # make sure all modules are in eval mode, onnx may change the training state\n",
    "    # of the module if the states are not consistent\n",
    "    def _check_eval(module):\n",
    "        assert not module.training\n",
    "\n",
    "    model.apply(_check_eval)\n",
    "\n",
    "    # Export the model to ONNX\n",
    "    with torch.no_grad():\n",
    "        with io.BytesIO() as f:\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                inputs,\n",
    "                f,\n",
    "                operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,\n",
    "                # verbose=True,  # NOTE: uncomment this for debugging\n",
    "                # export_params=True,\n",
    "                input_names=['image_batch', 'batched_inputs']\n",
    "            )\n",
    "            onnx_model = onnx.load_from_string(f.getvalue())\n",
    "\n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: ImageList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m export_onnx_model(model, (image_batch, input_dict))\n",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m             inputs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m             f,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m             operator_export_type\u001b[39m=\u001b[39;49mOperatorExportTypes\u001b[39m.\u001b[39;49mONNX_ATEN_FALLBACK,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m             \u001b[39m# verbose=True,  # NOTE: uncomment this for debugging\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m             \u001b[39m# export_params=True,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m             input_names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mimage_batch\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbatched_inputs\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         onnx_model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload_from_string(f\u001b[39m.\u001b[39mgetvalue())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X25sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m onnx_model\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     _export(\n\u001b[1;32m    517\u001b[0m         model,\n\u001b[1;32m    518\u001b[0m         args,\n\u001b[1;32m    519\u001b[0m         f,\n\u001b[1;32m    520\u001b[0m         export_params,\n\u001b[1;32m    521\u001b[0m         verbose,\n\u001b[1;32m    522\u001b[0m         training,\n\u001b[1;32m    523\u001b[0m         input_names,\n\u001b[1;32m    524\u001b[0m         output_names,\n\u001b[1;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[1;32m    533\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1597\u001b[0m     model,\n\u001b[1;32m   1598\u001b[0m     args,\n\u001b[1;32m   1599\u001b[0m     verbose,\n\u001b[1;32m   1600\u001b[0m     input_names,\n\u001b[1;32m   1601\u001b[0m     output_names,\n\u001b[1;32m   1602\u001b[0m     operator_export_type,\n\u001b[1;32m   1603\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1604\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1605\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1606\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1607\u001b[0m )\n\u001b[1;32m   1609\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1611\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1612\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    916\u001b[0m     model,\n\u001b[1;32m    917\u001b[0m     args,\n\u001b[1;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    921\u001b[0m )\n\u001b[1;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/jit/_trace.py:1285\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1284\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1285\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[1;32m   1286\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[1;32m   1287\u001b[0m )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1288\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/jit/_trace.py:100\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 100\u001b[0m     in_vars, in_desc \u001b[39m=\u001b[39m _flatten(args)\n\u001b[1;32m    101\u001b[0m     \u001b[39m# NOTE: use full state, because we need it for BatchNorm export\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# This differs from the compiler path, which doesn't support it at the moment.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     module_state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_unique_state_dict(\u001b[39mself\u001b[39m, keep_vars\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: ImageList"
     ]
    }
   ],
   "source": [
    "export_onnx_model(model, (image_batch, input_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Simple module for demonstration\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n",
    "\n",
    "module = MyModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx as onnx\n",
    "onnx.export(module, args=(inp), f=\"aba.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized = torch.ao.quantization.quantize_dynamic(model ,  # the original model\n",
    "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': [2, 444, 566], 'bbox': array([[112, 334, 444, 666],\n",
      "       [ 23,  35, 666, 435],\n",
      "       [ 22,  66, 343, 432]]), 'height': 1000, 'width': 1000}]\n",
      "tensor([[[[-1.7898, -1.8010, -1.7919,  ..., -1.7980, -1.7927, -1.7878],\n",
      "          [-1.7907, -1.8043, -1.7952,  ..., -1.8007, -1.7951, -1.8032],\n",
      "          [-1.7870, -1.8006, -1.7966,  ..., -1.8015, -1.7943, -1.8031],\n",
      "          ...,\n",
      "          [-1.7905, -1.7896, -1.8011,  ..., -1.7940, -1.7958, -1.7947],\n",
      "          [-1.8039, -1.7917, -1.7990,  ..., -1.7994, -1.7912, -1.7949],\n",
      "          [-1.8007, -1.8010, -1.7872,  ..., -1.7958, -1.7924, -1.7906]],\n",
      "\n",
      "         [[-2.0244, -2.0194, -2.0351,  ..., -2.0293, -2.0264, -2.0265],\n",
      "          [-2.0332, -2.0199, -2.0200,  ..., -2.0330, -2.0197, -2.0330],\n",
      "          [-2.0203, -2.0304, -2.0258,  ..., -2.0213, -2.0356, -2.0346],\n",
      "          ...,\n",
      "          [-2.0319, -2.0270, -2.0317,  ..., -2.0262, -2.0291, -2.0218],\n",
      "          [-2.0239, -2.0288, -2.0203,  ..., -2.0231, -2.0307, -2.0311],\n",
      "          [-2.0246, -2.0220, -2.0241,  ..., -2.0293, -2.0188, -2.0336]],\n",
      "\n",
      "         [[-2.1114, -2.1010, -2.1106,  ..., -2.1147, -2.1043, -2.1064],\n",
      "          [-2.1009, -2.1081, -2.1115,  ..., -2.1128, -2.1111, -2.1069],\n",
      "          [-2.1097, -2.1014, -2.1154,  ..., -2.1076, -2.1026, -2.1101],\n",
      "          ...,\n",
      "          [-2.1008, -2.1129, -2.1012,  ..., -2.1092, -2.1081, -2.1030],\n",
      "          [-2.1095, -2.1146, -2.1015,  ..., -2.1108, -2.1146, -2.1173],\n",
      "          [-2.1019, -2.1076, -2.1103,  ..., -2.1106, -2.1020, -2.1104]]]])\n",
      "torch.Size([1, 3, 1000, 1000])\n",
      "Batch_Size is >....... 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'weight' must be Tensor, not method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m quantized(image_batch, input_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:101\u001b[0m, in \u001b[0;36mVGT.forward\u001b[0;34m(self, image_batch, batched_inputs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(image_batch, batched_inputs)\n\u001b[1;32m    102\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batched_inputs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:161\u001b[0m, in \u001b[0;36mVGT.inference\u001b[0;34m(self, image_batch, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)11\u001b[39;00m\n\u001b[1;32m    160\u001b[0m chargrid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWordgrid_embedding(image_batch\u001b[39m.\u001b[39mtensor, batched_inputs)\n\u001b[0;32m--> 161\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(image_batch\u001b[39m.\u001b[39;49mtensor, chargrid)\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m detected_instances \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_generator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbackbone.py:211\u001b[0m, in \u001b[0;36mGridFPN.forward\u001b[0;34m(self, x, grid)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, grid):\n\u001b[1;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m        input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     bottom_up_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbottom_up(x, grid)\n\u001b[1;32m    212\u001b[0m     results \u001b[39m=\u001b[39m []\n\u001b[1;32m    213\u001b[0m     prev_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlateral_convs[\u001b[39m0\u001b[39m](bottom_up_features[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbackbone.py:184\u001b[0m, in \u001b[0;36mPTM_VIT_Backbone.forward\u001b[0;34m(self, x, grid)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39m    dict[str->Tensor]: names and the corresponding features\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    181\u001b[0m     x\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m\n\u001b[1;32m    182\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVIT takes an input of shape (N, C, H, W). Got \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m instead!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 184\u001b[0m vis_feat_out, grid_feat_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone\u001b[39m.\u001b[39;49mforward_features(x, grid)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFeatureMerge\u001b[39m.\u001b[39mforward(vis_feat_out, grid_feat_out)\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbeit.py:1167\u001b[0m, in \u001b[0;36mBEiT.forward_features\u001b[0;34m(self, x, grid)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mfor\u001b[39;00m i, blk \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_checkpoint:\n\u001b[0;32m-> 1167\u001b[0m         vis_x \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39;49mcheckpoint(\n\u001b[1;32m   1168\u001b[0m             blk, vis_x, rel_pos_bias, training_window_size\n\u001b[1;32m   1169\u001b[0m         )\n\u001b[1;32m   1170\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m         vis_x \u001b[39m=\u001b[39m blk(\n\u001b[1;32m   1172\u001b[0m             vis_x,\n\u001b[1;32m   1173\u001b[0m             rel_pos_bias\u001b[39m=\u001b[39mrel_pos_bias,\n\u001b[1;32m   1174\u001b[0m             training_window_size\u001b[39m=\u001b[39mtraining_window_size,\n\u001b[1;32m   1175\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/utils/checkpoint.py:451\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m context_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m noop_context_fn \u001b[39mor\u001b[39;00m debug \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39muse_reentrant=False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     gen \u001b[39m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    454\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    455\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/utils/checkpoint.py:230\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m    229\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 230\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbeit.py:637\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, rel_pos_bias, training_window_size)\u001b[0m\n\u001b[1;32m    633\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)))\n\u001b[1;32m    634\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma_1\n\u001b[0;32m--> 637\u001b[0m         \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    638\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x),\n\u001b[1;32m    639\u001b[0m             rel_pos_bias\u001b[39m=\u001b[39;49mrel_pos_bias,\n\u001b[1;32m    640\u001b[0m             training_window_size\u001b[39m=\u001b[39;49mtraining_window_size,\n\u001b[1;32m    641\u001b[0m         )\n\u001b[1;32m    642\u001b[0m     )\n\u001b[1;32m    643\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma_2 \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)))\n\u001b[1;32m    644\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGTbeit.py:464\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, rel_pos_bias, training_window_size)\u001b[0m\n\u001b[1;32m    456\u001b[0m     qkv_bias \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m    457\u001b[0m         (\n\u001b[1;32m    458\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m         )\n\u001b[1;32m    462\u001b[0m     )\n\u001b[1;32m    463\u001b[0m \u001b[39m# qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m qkv \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mx, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqkv\u001b[39m.\u001b[39;49mweight, bias\u001b[39m=\u001b[39;49mqkv_bias)\n\u001b[1;32m    465\u001b[0m qkv \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mreshape(B, N, \u001b[39m3\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m    466\u001b[0m q, k, v \u001b[39m=\u001b[39m (\n\u001b[1;32m    467\u001b[0m     qkv[\u001b[39m0\u001b[39m],\n\u001b[1;32m    468\u001b[0m     qkv[\u001b[39m1\u001b[39m],\n\u001b[1;32m    469\u001b[0m     qkv[\u001b[39m2\u001b[39m],\n\u001b[1;32m    470\u001b[0m )  \u001b[39m# make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'weight' must be Tensor, not method"
     ]
    }
   ],
   "source": [
    "quantized(image_batch, input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy(batched_inputs)\n",
      "Proxy(getattr_1)\n",
      "Proxy(size_1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (tuple, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/saeed/Walnut/Quantization/quantization.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# a tuple of one or more example inputs are needed to trace the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# example_inputs = (torch.rand(3,224,224))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# prepare\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m example_input \u001b[39m=\u001b[39m (image_batch, input_dict,)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model_prepared \u001b[39m=\u001b[39m quantize_fx\u001b[39m.\u001b[39;49mprepare_fx(model_to_quantize, qconfig_mapping, {\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_input})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# no calibration needed when we only have dynamic/weight_only quantization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# quantize\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/saeed/Walnut/Quantization/quantization.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m model_quantized \u001b[39m=\u001b[39m quantize_fx\u001b[39m.\u001b[39mconvert_fx(model_prepared)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:382\u001b[0m, in \u001b[0;36mprepare_fx\u001b[0;34m(model, qconfig_mapping, example_inputs, prepare_custom_config, _equalization_config, backend_config)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" Prepare a model for post training static quantization\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m    calibrate(prepared_model, sample_inference_data)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_once(\u001b[39m\"\u001b[39m\u001b[39mquantization_api.quantize_fx.prepare_fx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m _prepare_fx(\n\u001b[1;32m    383\u001b[0m     model,\n\u001b[1;32m    384\u001b[0m     qconfig_mapping,\n\u001b[1;32m    385\u001b[0m     \u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# is_qat\u001b[39;49;00m\n\u001b[1;32m    386\u001b[0m     example_inputs,\n\u001b[1;32m    387\u001b[0m     prepare_custom_config,\n\u001b[1;32m    388\u001b[0m     _equalization_config,\n\u001b[1;32m    389\u001b[0m     backend_config,\n\u001b[1;32m    390\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py:135\u001b[0m, in \u001b[0;36m_prepare_fx\u001b[0;34m(model, qconfig_mapping, is_qat, example_inputs, prepare_custom_config, _equalization_config, backend_config, is_standalone_module)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m# symbolically trace the model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m tracer \u001b[39m=\u001b[39m QuantizationTracer(skipped_module_names, skipped_module_classes)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m graph_module \u001b[39m=\u001b[39m GraphModule(model, tracer\u001b[39m.\u001b[39;49mtrace(model))\n\u001b[1;32m    136\u001b[0m _attach_meta_to_node_if_not_exist(graph_module)\n\u001b[1;32m    138\u001b[0m fuse_custom_config \u001b[39m=\u001b[39m FuseCustomConfig()\u001b[39m.\u001b[39mset_preserved_attributes(prepare_custom_config\u001b[39m.\u001b[39mpreserved_attributes)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:817\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_search:\n\u001b[1;32m    811\u001b[0m             _autowrap_check(\n\u001b[1;32m    812\u001b[0m                 patcher, module\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    814\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_node(\n\u001b[1;32m    815\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    816\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 817\u001b[0m             (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_arg(fn(\u001b[39m*\u001b[39;49margs)),),\n\u001b[1;32m    818\u001b[0m             {},\n\u001b[1;32m    819\u001b[0m             type_expr\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mreturn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    820\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmodule_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:101\u001b[0m, in \u001b[0;36mVGT.forward\u001b[0;34m(self, image_batch, batched_inputs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(image_batch, batched_inputs)\n\u001b[1;32m    102\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batched_inputs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/VGT.py:160\u001b[0m, in \u001b[0;36mVGT.inference\u001b[0;34m(self, image_batch, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n\u001b[1;32m    159\u001b[0m \u001b[39m# images = self.preprocess_image(batched_inputs)11\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m chargrid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWordgrid_embedding(image_batch\u001b[39m.\u001b[39;49mtensor, batched_inputs)\n\u001b[1;32m    161\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(image_batch\u001b[39m.\u001b[39mtensor, chargrid)\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m detected_instances \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:795\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m _autowrap_check(\n\u001b[1;32m    791\u001b[0m     patcher,\n\u001b[1;32m    792\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(mod, \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m, mod), \u001b[39m\"\u001b[39m\u001b[39m__globals__\u001b[39m\u001b[39m\"\u001b[39m, {}),\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_module(mod, forward, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:479\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_stack[_scope\u001b[39m.\u001b[39mmodule_path] \u001b[39m=\u001b[39m _scope\u001b[39m.\u001b[39mmodule_type\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 479\u001b[0m     ret_val \u001b[39m=\u001b[39m forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m     ret_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_proxy(\u001b[39m\"\u001b[39m\u001b[39mcall_module\u001b[39m\u001b[39m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:788\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 788\u001b[0m     \u001b[39mreturn\u001b[39;00m _orig_module_call(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Walnut/Quantization/ai/ditod/Wordnn_embedding.py:77\u001b[0m, in \u001b[0;36mWordnnEmbedding.forward\u001b[0;34m(self, img, batched_inputs, stride)\u001b[0m\n\u001b[1;32m     74\u001b[0m batch_b, _, batch_h, batch_w \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize()\n\u001b[1;32m     75\u001b[0m \u001b[39mprint\u001b[39m(img\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> 77\u001b[0m chargrid_map \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\n\u001b[1;32m     78\u001b[0m     (batch_b, batch_h \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m stride, batch_w \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m stride), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint64\n\u001b[1;32m     79\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBatch_Size is >.......\u001b[39m\u001b[39m\"\u001b[39m, batch_b)\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m iter_b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_b):\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (tuple, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "\n",
    "\n",
    "#\n",
    "# post training dynamic/weight_only quantization\n",
    "#\n",
    "\n",
    "# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\n",
    "model_to_quantize = copy.deepcopy(model)\n",
    "model_to_quantize.eval()\n",
    "qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n",
    "# a tuple of one or more example inputs are needed to trace the model\n",
    "# example_inputs = (torch.rand(3,224,224))\n",
    "# prepare\n",
    "example_input = (image_batch, input_dict,)\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, {\"\": example_input})\n",
    "# no calibration needed when we only have dynamic/weight_only quantization\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(list(model.children())[0].children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/saeed/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|| 160M/160M [00:11<00:00, 14.1MB/s] \n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/nn/functional.py:3964: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/ops/boxes.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/ops/boxes.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/__init__.py:1404: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/detection/transform.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torchvision/models/detection/transform.py:307: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "/home/saeed/anaconda3/envs/VGT/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5856: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dummpy_input = torch.randn(1, 3, 1333, 800)\n",
    "model.eval()\n",
    "model(dummpy_input)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  dummpy_input,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"model.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
