{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeneralizedRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from detectron2.config import configurable\n",
    "from detectron2.data.detection_utils import convert_image_to_rgb\n",
    "from detectron2.layers import move_device_like\n",
    "from detectron2.structures import ImageList, Instances\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.utils.logger import log_first_n\n",
    "\n",
    "from detectron2.modeling.backbone import Backbone, build_backbone\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.modeling.proposal_generator import build_proposal_generator\n",
    "from detectron2.modeling.roi_heads import build_roi_heads\n",
    "from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n",
    "class GeneralizedRCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalized R-CNN. Any models that contains the following three components:\n",
    "    1. Per-image feature extraction (aka backbone)\n",
    "    2. Region proposal generation\n",
    "    3. Per-region feature extraction and prediction\n",
    "    \"\"\"\n",
    "\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        backbone: Backbone,\n",
    "        proposal_generator: nn.Module,\n",
    "        roi_heads: nn.Module,\n",
    "        pixel_mean: Tuple[float],\n",
    "        pixel_std: Tuple[float],\n",
    "        input_format: Optional[str] = None,\n",
    "        vis_period: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone: a backbone module, must follow detectron2's backbone interface\n",
    "            proposal_generator: a module that generates proposals using backbone features\n",
    "            roi_heads: a ROI head that performs per-region computation\n",
    "            pixel_mean, pixel_std: list or tuple with #channels element, representing\n",
    "                the per-channel mean and std to be used to normalize the input image\n",
    "            input_format: describe the meaning of channels of input. Needed by visualization\n",
    "            vis_period: the period to run visualization. Set to 0 to disable.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proposal_generator = proposal_generator\n",
    "        self.roi_heads = roi_heads\n",
    "\n",
    "        self.input_format = input_format\n",
    "        self.vis_period = vis_period\n",
    "        if vis_period > 0:\n",
    "            assert input_format is not None, \"input_format is required for visualization!\"\n",
    "\n",
    "        self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1), False)\n",
    "        assert (\n",
    "            self.pixel_mean.shape == self.pixel_std.shape\n",
    "        ), f\"{self.pixel_mean} and {self.pixel_std} have different shapes!\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        backbone = build_backbone(cfg)\n",
    "        return {\n",
    "            \"backbone\": backbone,\n",
    "            \"proposal_generator\": build_proposal_generator(cfg, backbone.output_shape()),\n",
    "            \"roi_heads\": build_roi_heads(cfg, backbone.output_shape()),\n",
    "            \"input_format\": cfg.INPUT.FORMAT,\n",
    "            \"vis_period\": cfg.VIS_PERIOD,\n",
    "            \"pixel_mean\": cfg.MODEL.PIXEL_MEAN,\n",
    "            \"pixel_std\": cfg.MODEL.PIXEL_STD,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.pixel_mean.device\n",
    "\n",
    "    def _move_to_current_device(self, x):\n",
    "        return move_device_like(x, self.pixel_mean)\n",
    "\n",
    "    def visualize_training(self, batched_inputs, proposals):\n",
    "        \"\"\"\n",
    "        A function used to visualize images and proposals. It shows ground truth\n",
    "        bounding boxes on the original image and up to 20 top-scoring predicted\n",
    "        object proposals on the original image. Users can implement different\n",
    "        visualization functions for different models.\n",
    "\n",
    "        Args:\n",
    "            batched_inputs (list): a list that contains input to the model.\n",
    "            proposals (list): a list that contains predicted proposals. Both\n",
    "                batched_inputs and proposals should have the same length.\n",
    "        \"\"\"\n",
    "        from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "        storage = get_event_storage()\n",
    "        max_vis_prop = 20\n",
    "\n",
    "        for input, prop in zip(batched_inputs, proposals):\n",
    "            img = input[\"image\"]\n",
    "            img = convert_image_to_rgb(img.permute(1, 2, 0), self.input_format)\n",
    "            v_gt = Visualizer(img, None)\n",
    "            v_gt = v_gt.overlay_instances(boxes=input[\"instances\"].gt_boxes)\n",
    "            anno_img = v_gt.get_image()\n",
    "            box_size = min(len(prop.proposal_boxes), max_vis_prop)\n",
    "            v_pred = Visualizer(img, None)\n",
    "            v_pred = v_pred.overlay_instances(\n",
    "                boxes=prop.proposal_boxes[0:box_size].tensor.cpu().numpy()\n",
    "            )\n",
    "            prop_img = v_pred.get_image()\n",
    "            vis_img = np.concatenate((anno_img, prop_img), axis=1)\n",
    "            vis_img = vis_img.transpose(2, 0, 1)\n",
    "            vis_name = \"Left: GT bounding boxes;  Right: Predicted proposals\"\n",
    "            storage.put_image(vis_name, vis_img)\n",
    "            break  # only visualize one image in a batch\n",
    "\n",
    "    def forward(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n",
    "                Each item in the list contains the inputs for one image.\n",
    "                For now, each item in the list is a dict that contains:\n",
    "\n",
    "                * image: Tensor, image in (C, H, W) format.\n",
    "                * instances (optional): groundtruth :class:`Instances`\n",
    "                * proposals (optional): :class:`Instances`, precomputed proposals.\n",
    "\n",
    "                Other information that's included in the original dicts, such as:\n",
    "\n",
    "                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n",
    "                  See :meth:`postprocess` for details.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]:\n",
    "                Each dict is the output for one input image.\n",
    "                The dict contains one key \"instances\" whose value is a :class:`Instances`.\n",
    "                The :class:`Instances` object has the following keys:\n",
    "                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return self.inference(batched_inputs)\n",
    "\n",
    "        images = self.preprocess_image(batched_inputs)\n",
    "        if \"instances\" in batched_inputs[0]:\n",
    "            gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "        else:\n",
    "            gt_instances = None\n",
    "\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        if self.proposal_generator is not None:\n",
    "            proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)\n",
    "        else:\n",
    "            assert \"proposals\" in batched_inputs[0]\n",
    "            proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n",
    "            proposal_losses = {}\n",
    "\n",
    "        _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n",
    "        if self.vis_period > 0:\n",
    "            storage = get_event_storage()\n",
    "            if storage.iter % self.vis_period == 0:\n",
    "                self.visualize_training(batched_inputs, proposals)\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "        return losses\n",
    "\n",
    "    def inference(\n",
    "        self,\n",
    "        batched_inputs: List[Dict[str, torch.Tensor]],\n",
    "        detected_instances: Optional[List[Instances]] = None,\n",
    "        do_postprocess: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run inference on the given inputs.\n",
    "\n",
    "        Args:\n",
    "            batched_inputs (list[dict]): same as in :meth:`forward`\n",
    "            detected_instances (None or list[Instances]): if not None, it\n",
    "                contains an `Instances` object per image. The `Instances`\n",
    "                object contains \"pred_boxes\" and \"pred_classes\" which are\n",
    "                known boxes in the image.\n",
    "                The inference will then skip the detection of bounding boxes,\n",
    "                and only predict other per-ROI outputs.\n",
    "            do_postprocess (bool): whether to apply post-processing on the outputs.\n",
    "\n",
    "        Returns:\n",
    "            When do_postprocess=True, same as in :meth:`forward`.\n",
    "            Otherwise, a list[Instances] containing raw network outputs.\n",
    "        \"\"\"\n",
    "        assert not self.training\n",
    "\n",
    "        images = self.preprocess_image(batched_inputs)\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        if detected_instances is None:\n",
    "            if self.proposal_generator is not None:\n",
    "                proposals, _ = self.proposal_generator(images, features, None)\n",
    "            else:\n",
    "                assert \"proposals\" in batched_inputs[0]\n",
    "                proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n",
    "\n",
    "            results, _ = self.roi_heads(images, features, proposals, None)\n",
    "        else:\n",
    "            detected_instances = [x.to(self.device) for x in detected_instances]\n",
    "            results = self.roi_heads.forward_with_given_boxes(features, detected_instances)\n",
    "\n",
    "        if do_postprocess:\n",
    "            assert not torch.jit.is_scripting(), \"Scripting is not supported for postprocess.\"\n",
    "            return GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)\n",
    "        return results\n",
    "\n",
    "    def preprocess_image(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n",
    "        \"\"\"\n",
    "        Normalize, pad and batch the input images.\n",
    "        \"\"\"\n",
    "        images = [self._move_to_current_device(x[\"image\"]) for x in batched_inputs]\n",
    "        images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n",
    "        images = ImageList.from_tensors(\n",
    "            images,\n",
    "            self.backbone.size_divisibility,\n",
    "            padding_constraints=self.backbone.padding_constraints,\n",
    "        )\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def _postprocess(instances, batched_inputs: List[Dict[str, torch.Tensor]], image_sizes):\n",
    "        \"\"\"\n",
    "        Rescale the output instances to the target size.\n",
    "        \"\"\"\n",
    "        # note: private function; subject to changes\n",
    "        processed_results = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            instances, batched_inputs, image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results.append({\"instances\": r})\n",
    "        return processed_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from detectron2.config import configurable\n",
    "from detectron2.layers import ShapeSpec, move_device_like\n",
    "from detectron2.structures import Boxes, RotatedBoxes\n",
    "from detectron2.utils.registry import Registry\n",
    "class BufferList(nn.Module):\n",
    "    \"\"\"\n",
    "    Similar to nn.ParameterList, but for buffers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffers):\n",
    "        super().__init__()\n",
    "        for i, buffer in enumerate(buffers):\n",
    "            # Use non-persistent buffer so the values are not saved in checkpoint\n",
    "            self.register_buffer(str(i), buffer, persistent=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buffers)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._buffers.values())\n",
    "\n",
    "\n",
    "def _create_grid_offsets(\n",
    "    size: List[int], stride: int, offset: float, target_device_tensor: torch.Tensor\n",
    "):\n",
    "    grid_height, grid_width = size\n",
    "    shifts_x = move_device_like(\n",
    "        torch.arange(offset * stride, grid_width * stride, step=stride, dtype=torch.float32),\n",
    "        target_device_tensor,\n",
    "    )\n",
    "    shifts_y = move_device_like(\n",
    "        torch.arange(offset * stride, grid_height * stride, step=stride, dtype=torch.float32),\n",
    "        target_device_tensor,\n",
    "    )\n",
    "\n",
    "    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n",
    "    shift_x = shift_x.reshape(-1)\n",
    "    shift_y = shift_y.reshape(-1)\n",
    "    return shift_x, shift_y\n",
    "\n",
    "\n",
    "def _broadcast_params(params, num_features, name):\n",
    "    \"\"\"\n",
    "    If one size (or aspect ratio) is specified and there are multiple feature\n",
    "    maps, we \"broadcast\" anchors of that single size (or aspect ratio)\n",
    "    over all feature maps.\n",
    "\n",
    "    If params is list[float], or list[list[float]] with len(params) == 1, repeat\n",
    "    it num_features time.\n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: param for each feature\n",
    "    \"\"\"\n",
    "    assert isinstance(\n",
    "        params, collections.abc.Sequence\n",
    "    ), f\"{name} in anchor generator has to be a list! Got {params}.\"\n",
    "    assert len(params), f\"{name} in anchor generator cannot be empty!\"\n",
    "    if not isinstance(params[0], collections.abc.Sequence):  # params is list[float]\n",
    "        return [params] * num_features\n",
    "    if len(params) == 1:\n",
    "        return list(params) * num_features\n",
    "    assert len(params) == num_features, (\n",
    "        f\"Got {name} of length {len(params)} in anchor generator, \"\n",
    "        f\"but the number of input features is {num_features}!\"\n",
    "    )\n",
    "    return params\n",
    "\n",
    "class DefaultAnchorGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute anchors in the standard ways described in\n",
    "    \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\".\n",
    "    \"\"\"\n",
    "\n",
    "    box_dim: torch.jit.Final[int] = 4\n",
    "    \"\"\"\n",
    "    the dimension of each anchor box.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, input_shape: List[ShapeSpec], offset=0.5):\n",
    "        \"\"\"\n",
    "        This interface is experimental.\n",
    "\n",
    "        Args:\n",
    "            sizes (list[list[float]] or list[float]):\n",
    "                If ``sizes`` is list[list[float]], ``sizes[i]`` is the list of anchor sizes\n",
    "                (i.e. sqrt of anchor area) to use for the i-th feature map.\n",
    "                If ``sizes`` is list[float], ``sizes`` is used for all feature maps.\n",
    "                Anchor sizes are given in absolute lengths in units of\n",
    "                the input image; they do not dynamically scale if the input image size changes.\n",
    "            aspect_ratios (list[list[float]] or list[float]): list of aspect ratios\n",
    "                (i.e. height / width) to use for anchors. Same \"broadcast\" rule for `sizes` applies.\n",
    "            strides (list[int]): stride of each input feature.\n",
    "            offset (float): Relative offset between the center of the first anchor and the top-left\n",
    "                corner of the image. Value has to be in [0, 1).\n",
    "                Recommend to use 0.5, which means half stride.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.strides = [x.stride for x in input_shape]\n",
    "        self.num_features = len(self.strides)\n",
    "        sizes_ = cfg.MODEL.ANCHOR_GENERATOR.SIZES\n",
    "        aspect_ratios = cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS\n",
    "        sizes = _broadcast_params(sizes_, self.num_features, \"sizes\")\n",
    "        aspect_ratios = _broadcast_params(aspect_ratios, self.num_features, \"aspect_ratios\")\n",
    "        self.cell_anchors = self._calculate_anchors(sizes, aspect_ratios)\n",
    "\n",
    "        self.offset = cfg.MODEL.ANCHOR_GENERATOR.OFFSET\n",
    "        assert 0.0 <= self.offset < 1.0, self.offset\n",
    "\n",
    "    def _calculate_anchors(self, sizes, aspect_ratios):\n",
    "        cell_anchors = [\n",
    "            self.generate_cell_anchors(s, a).float() for s, a in zip(sizes, aspect_ratios)\n",
    "        ]\n",
    "        return BufferList(cell_anchors)\n",
    "\n",
    "    @property\n",
    "    @torch.jit.unused\n",
    "    def num_cell_anchors(self):\n",
    "        \"\"\"\n",
    "        Alias of `num_anchors`.\n",
    "        \"\"\"\n",
    "        return self.num_anchors\n",
    "\n",
    "    @property\n",
    "    @torch.jit.unused\n",
    "    def num_anchors(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list[int]: Each int is the number of anchors at every pixel\n",
    "                location, on that feature map.\n",
    "                For example, if at every pixel we use anchors of 3 aspect\n",
    "                ratios and 5 sizes, the number of anchors is 15.\n",
    "                (See also ANCHOR_GENERATOR.SIZES and ANCHOR_GENERATOR.ASPECT_RATIOS in config)\n",
    "\n",
    "                In standard RPN models, `num_anchors` on every feature map is the same.\n",
    "        \"\"\"\n",
    "        return [len(cell_anchors) for cell_anchors in self.cell_anchors]\n",
    "\n",
    "    def _grid_anchors(self, grid_sizes: List[List[int]]):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list[Tensor]: #featuremap tensors, each is (#locations x #cell_anchors) x 4\n",
    "        \"\"\"\n",
    "        anchors = []\n",
    "        # buffers() not supported by torchscript. use named_buffers() instead\n",
    "        buffers: List[torch.Tensor] = [x[1] for x in self.cell_anchors.named_buffers()]\n",
    "        for size, stride, base_anchors in zip(grid_sizes, self.strides, buffers):\n",
    "            shift_x, shift_y = _create_grid_offsets(size, stride, self.offset, base_anchors)\n",
    "            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)\n",
    "\n",
    "            anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def generate_cell_anchors(self, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)):\n",
    "        \"\"\"\n",
    "        Generate a tensor storing canonical anchor boxes, which are all anchor\n",
    "        boxes of different sizes and aspect_ratios centered at (0, 0).\n",
    "        We can later build the set of anchors for a full feature map by\n",
    "        shifting and tiling these tensors (see `meth:_grid_anchors`).\n",
    "\n",
    "        Args:\n",
    "            sizes (tuple[float]):\n",
    "            aspect_ratios (tuple[float]]):\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (len(sizes) * len(aspect_ratios), 4) storing anchor boxes\n",
    "                in XYXY format.\n",
    "        \"\"\"\n",
    "\n",
    "        # This is different from the anchor generator defined in the original Faster R-CNN\n",
    "        # code or Detectron. They yield the same AP, however the old version defines cell\n",
    "        # anchors in a less natural way with a shift relative to the feature grid and\n",
    "        # quantization that results in slightly different sizes for different aspect ratios.\n",
    "        # See also https://github.com/facebookresearch/Detectron/issues/227\n",
    "\n",
    "        anchors = []\n",
    "        for size in sizes:\n",
    "            area = size**2.0\n",
    "            for aspect_ratio in aspect_ratios:\n",
    "                # s * s = w * h\n",
    "                # a = h / w\n",
    "                # ... some algebra ...\n",
    "                # w = sqrt(s * s / a)\n",
    "                # h = a * w\n",
    "                w = math.sqrt(area / aspect_ratio)\n",
    "                h = aspect_ratio * w\n",
    "                x0, y0, x1, y1 = -w / 2.0, -h / 2.0, w / 2.0, h / 2.0\n",
    "                anchors.append([x0, y0, x1, y1])\n",
    "        return torch.tensor(anchors)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (list[Tensor]): list of backbone feature maps on which to generate anchors.\n",
    "\n",
    "        Returns:\n",
    "            list[Boxes]: a list of Boxes containing all the anchors for each feature map\n",
    "                (i.e. the cell anchors repeated over all locations in the feature map).\n",
    "                The number of anchors of each feature map is Hi x Wi x num_cell_anchors,\n",
    "                where Hi, Wi are resolution of the feature map divided by anchor stride.\n",
    "        \"\"\"\n",
    "        grid_sizes = [feature_map.shape[-2:] for feature_map in features]\n",
    "        anchors_over_all_feature_maps = self._grid_anchors(grid_sizes)\n",
    "        return [Boxes(x) for x in anchors_over_all_feature_maps]\n",
    "\n",
    "def anchor_generator_custom(cfg, input_shape):\n",
    "    # input_shapes = input_shape.values()\n",
    "    return DefaultAnchorGenerator(cfg, input_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPN Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from detectron2.config import configurable\n",
    "from detectron2.layers import Conv2d, ShapeSpec, cat\n",
    "from detectron2.structures import Boxes, ImageList, Instances, pairwise_iou\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.utils.memory import retry_if_cuda_oom\n",
    "from detectron2.utils.registry import Registry\n",
    "from detectron2.modeling.anchor_generator import build_anchor_generator\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform, _dense_box_regression_loss\n",
    "from detectron2.modeling.matcher import Matcher\n",
    "from detectron2.modeling.sampling import subsample_labels\n",
    "from detectron2.modeling.proposal_generator.build import PROPOSAL_GENERATOR_REGISTRY\n",
    "from detectron2.modeling.proposal_generator.proposal_utils import find_top_rpn_proposals\n",
    "\n",
    "class StandardRPNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard RPN classification and regression heads described in :paper:`Faster R-CNN`.\n",
    "    Uses a 3x3 conv to produce a shared hidden state from which one 1x1 conv predicts\n",
    "    objectness logits for each anchor and a second 1x1 conv predicts bounding-box deltas\n",
    "    specifying how to deform each anchor into an object proposal.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, cfg, input_shape): # conv_dims: List[int] = (-1,)\n",
    "    #):\n",
    "        \"\"\"\n",
    "        NOTE: this interface is experimental.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of input feature channels. When using multiple\n",
    "                input features, they must have the same number of channels.\n",
    "            num_anchors (int): number of anchors to predict for *each spatial position*\n",
    "                on the feature map. The total number of anchors for each\n",
    "                feature map will be `num_anchors * H * W`.\n",
    "            box_dim (int): dimension of a box, which is also the number of box regression\n",
    "                predictions to make for each anchor. An axis aligned box has\n",
    "                box_dim=4, while a rotated box has box_dim=5.\n",
    "            conv_dims (list[int]): a list of integers representing the output channels\n",
    "                of N conv layers. Set it to -1 to use the same number of output channels\n",
    "                as input channels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        in_channels = [s.channels for s in input_shape]\n",
    "        assert len(set(in_channels)) == 1, \"Each level must have the same channel!\"\n",
    "        in_channels = in_channels[0]\n",
    "        cur_channels = in_channels\n",
    "        anchor_generator = anchor_generator_custom(cfg, input_shape)\n",
    "        num_anchors = anchor_generator.num_anchors\n",
    "        assert (len(set(num_anchors)) == 1), \"Each level must have the same number of anchors per spatial position\"\n",
    "        num_anchors = num_anchors[0]\n",
    "        box_dim = anchor_generator.box_dim\n",
    "\n",
    "\n",
    "        # Keeping the old variable names and structure for backwards compatiblity.\n",
    "        # Otherwise the old checkpoints will fail to load.\n",
    "        conv_dims = cfg.MODEL.RPN.CONV_DIMS\n",
    "        if len(conv_dims) == 1:\n",
    "            out_channels = cur_channels if conv_dims[0] == -1 else conv_dims[0]\n",
    "            print(in_channels,\"/n\", out_channels)\n",
    "\n",
    "            # 3x3 conv for the hidden representation\n",
    "            self.conv = self._get_rpn_conv(cur_channels, out_channels)\n",
    "            cur_channels = out_channels\n",
    "        else:\n",
    "            self.conv = nn.Sequential()\n",
    "            for k, conv_dim in enumerate(conv_dims):\n",
    "                out_channels = cur_channels if conv_dim == -1 else conv_dim\n",
    "                print(in_channels,\"/n\", out_channels)\n",
    "\n",
    "                if out_channels <= 0:\n",
    "                    raise ValueError(\n",
    "                        f\"Conv output channels should be greater than 0. Got {out_channels}\"\n",
    "                    )\n",
    "                conv = self._get_rpn_conv(cur_channels, out_channels)\n",
    "                self.conv.add_module(f\"conv{k}\", conv)\n",
    "                cur_channels = out_channels\n",
    "        # 1x1 conv for predicting objectness logits\n",
    "        self.objectness_logits = nn.Conv2d(cur_channels, num_anchors, kernel_size=1, stride=1)\n",
    "        # 1x1 conv for predicting box2box transform deltas\n",
    "        self.anchor_deltas = nn.Conv2d(cur_channels, num_anchors * box_dim, kernel_size=1, stride=1)\n",
    "\n",
    "        # Keeping the order of weights initialization same for backwards compatiblility.\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, std=0.01)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def _get_rpn_conv(self, in_channels, out_channels):\n",
    "        return Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            activation=nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (list[Tensor]): list of feature maps\n",
    "\n",
    "        Returns:\n",
    "            list[Tensor]: A list of L elements.\n",
    "                Element i is a tensor of shape (N, A, Hi, Wi) representing\n",
    "                the predicted objectness logits for all anchors. A is the number of cell anchors.\n",
    "            list[Tensor]: A list of L elements. Element i is a tensor of shape\n",
    "                (N, A*box_dim, Hi, Wi) representing the predicted \"deltas\" used to transform anchors\n",
    "                to proposals.\n",
    "        \"\"\"\n",
    "        pred_objectness_logits = []\n",
    "        pred_anchor_deltas = []\n",
    "        for x in features:\n",
    "            t = self.conv(x)\n",
    "            pred_objectness_logits.append(self.objectness_logits(t))\n",
    "            pred_anchor_deltas.append(self.anchor_deltas(t))\n",
    "        return pred_objectness_logits, pred_anchor_deltas\n",
    "    \n",
    "def build_rpn_head_custom(cfg, input_shapes):\n",
    "    return StandardRPNHead(cfg, input_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "\n",
    "        # ret[\"anchor_generator\"] = build_anchor_generator(cfg, [input_shape[f] for f in \n",
    "        # ret[\"head\"] = build_rpn_head(cfg, [input_shape[f] for f in in_features])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        input_shape\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = cfg.MODEL.RPN.IN_FEATURES\n",
    "        self.rpn_head = build_rpn_head_custom(cfg, input_shape)\n",
    "        self.anchor_generator = anchor_generator_custom(cfg, input_shape)\n",
    "        self.anchor_matcher = Matcher(\n",
    "            cfg.MODEL.RPN.IOU_THRESHOLDS, cfg.MODEL.RPN.IOU_LABELS, allow_low_quality_matches=True\n",
    "        )\n",
    "        self.box2box_transform = Box2BoxTransform(weights=cfg.MODEL.RPN.BBOX_REG_WEIGHTS)\n",
    "        self.batch_size_per_image = cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE\n",
    "        self.positive_fraction = cfg.MODEL.RPN.POSITIVE_FRACTION\n",
    "        # Map from self.training state to train/test settings\n",
    "        self.pre_nms_topk = {True: cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN, False: cfg.MODEL.RPN.PRE_NMS_TOPK_TEST}\n",
    "        self.post_nms_topk = {True: cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN, False: cfg.MODEL.RPN.POST_NMS_TOPK_TEST}\n",
    "        self.nms_thresh = cfg.MODEL.RPN.NMS_THRESH\n",
    "        self.min_box_size = float(cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE)\n",
    "        self.anchor_boundary_thresh = cfg.MODEL.RPN.BOUNDARY_THRESH\n",
    "        loss_weight = {\n",
    "                \"loss_rpn_cls\": cfg.MODEL.RPN.LOSS_WEIGHT,\n",
    "                \"loss_rpn_loc\": cfg.MODEL.RPN.BBOX_REG_LOSS_WEIGHT * cfg.MODEL.RPN.LOSS_WEIGHT,\n",
    "            }\n",
    "        self.loss_weight = loss_weight\n",
    "        self.box_reg_loss_type = cfg.MODEL.RPN.BBOX_REG_LOSS_TYPE\n",
    "        self.smooth_l1_beta = cfg.MODEL.RPN.SMOOTH_L1_BETA\n",
    "\n",
    "    def _subsample_labels(self, label):\n",
    "        \"\"\"\n",
    "        Randomly sample a subset of positive and negative examples, and overwrite\n",
    "        the label vector to the ignore value (-1) for all elements that are not\n",
    "        included in the sample.\n",
    "\n",
    "        Args:\n",
    "            labels (Tensor): a vector of -1, 0, 1. Will be modified in-place and returned.\n",
    "        \"\"\"\n",
    "        pos_idx, neg_idx = subsample_labels(\n",
    "            label, self.batch_size_per_image, self.positive_fraction, 0\n",
    "        )\n",
    "        # Fill with the ignore label (-1), then set positive and negative labels\n",
    "        label.fill_(-1)\n",
    "        label.scatter_(0, pos_idx, 1)\n",
    "        label.scatter_(0, neg_idx, 0)\n",
    "        return label\n",
    "\n",
    "    @torch.jit.unused\n",
    "    @torch.no_grad()\n",
    "    def label_and_sample_anchors(\n",
    "        self, anchors: List[Boxes], gt_instances: List[Instances]\n",
    "    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            anchors (list[Boxes]): anchors for each feature map.\n",
    "            gt_instances: the ground-truth instances for each image.\n",
    "\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                List of #img tensors. i-th element is a vector of labels whose length is\n",
    "                the total number of anchors across all feature maps R = sum(Hi * Wi * A).\n",
    "                Label values are in {-1, 0, 1}, with meanings: -1 = ignore; 0 = negative\n",
    "                class; 1 = positive class.\n",
    "            list[Tensor]:\n",
    "                i-th element is a Rx4 tensor. The values are the matched gt boxes for each\n",
    "                anchor. Values are undefined for those anchors not labeled as 1.\n",
    "        \"\"\"\n",
    "        anchors = Boxes.cat(anchors)\n",
    "\n",
    "        gt_boxes = [x.gt_boxes for x in gt_instances]\n",
    "        image_sizes = [x.image_size for x in gt_instances]\n",
    "        del gt_instances\n",
    "\n",
    "        gt_labels = []\n",
    "        matched_gt_boxes = []\n",
    "        for image_size_i, gt_boxes_i in zip(image_sizes, gt_boxes):\n",
    "            \"\"\"\n",
    "            image_size_i: (h, w) for the i-th image\n",
    "            gt_boxes_i: ground-truth boxes for i-th image\n",
    "            \"\"\"\n",
    "\n",
    "            match_quality_matrix = retry_if_cuda_oom(pairwise_iou)(gt_boxes_i, anchors)\n",
    "            matched_idxs, gt_labels_i = retry_if_cuda_oom(self.anchor_matcher)(match_quality_matrix)\n",
    "            # Matching is memory-expensive and may result in CPU tensors. But the result is small\n",
    "            gt_labels_i = gt_labels_i.to(device=gt_boxes_i.device)\n",
    "            del match_quality_matrix\n",
    "\n",
    "            if self.anchor_boundary_thresh >= 0:\n",
    "                # Discard anchors that go out of the boundaries of the image\n",
    "                # NOTE: This is legacy functionality that is turned off by default in Detectron2\n",
    "                anchors_inside_image = anchors.inside_box(image_size_i, self.anchor_boundary_thresh)\n",
    "                gt_labels_i[~anchors_inside_image] = -1\n",
    "\n",
    "            # A vector of labels (-1, 0, 1) for each anchor\n",
    "            gt_labels_i = self._subsample_labels(gt_labels_i)\n",
    "\n",
    "            if len(gt_boxes_i) == 0:\n",
    "                # These values won't be used anyway since the anchor is labeled as background\n",
    "                matched_gt_boxes_i = torch.zeros_like(anchors.tensor)\n",
    "            else:\n",
    "                # TODO wasted indexing computation for ignored boxes\n",
    "                matched_gt_boxes_i = gt_boxes_i[matched_idxs].tensor\n",
    "\n",
    "            gt_labels.append(gt_labels_i)  # N,AHW\n",
    "            matched_gt_boxes.append(matched_gt_boxes_i)\n",
    "        return gt_labels, matched_gt_boxes\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def losses(\n",
    "        self,\n",
    "        anchors: List[Boxes],\n",
    "        pred_objectness_logits: List[torch.Tensor],\n",
    "        gt_labels: List[torch.Tensor],\n",
    "        pred_anchor_deltas: List[torch.Tensor],\n",
    "        gt_boxes: List[torch.Tensor],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return the losses from a set of RPN predictions and their associated ground-truth.\n",
    "\n",
    "        Args:\n",
    "            anchors (list[Boxes or RotatedBoxes]): anchors for each feature map, each\n",
    "                has shape (Hi*Wi*A, B), where B is box dimension (4 or 5).\n",
    "            pred_objectness_logits (list[Tensor]): A list of L elements.\n",
    "                Element i is a tensor of shape (N, Hi*Wi*A) representing\n",
    "                the predicted objectness logits for all anchors.\n",
    "            gt_labels (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n",
    "            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a tensor of shape\n",
    "                (N, Hi*Wi*A, 4 or 5) representing the predicted \"deltas\" used to transform anchors\n",
    "                to proposals.\n",
    "            gt_boxes (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n",
    "\n",
    "        Returns:\n",
    "            dict[loss name -> loss value]: A dict mapping from loss name to loss value.\n",
    "                Loss names are: `loss_rpn_cls` for objectness classification and\n",
    "                `loss_rpn_loc` for proposal localization.\n",
    "        \"\"\"\n",
    "        num_images = len(gt_labels)\n",
    "        gt_labels = torch.stack(gt_labels)  # (N, sum(Hi*Wi*Ai))\n",
    "\n",
    "        # Log the number of positive/negative anchors per-image that's used in training\n",
    "        pos_mask = gt_labels == 1\n",
    "        num_pos_anchors = pos_mask.sum().item()\n",
    "        num_neg_anchors = (gt_labels == 0).sum().item()\n",
    "        storage = get_event_storage()\n",
    "        storage.put_scalar(\"rpn/num_pos_anchors\", num_pos_anchors / num_images)\n",
    "        storage.put_scalar(\"rpn/num_neg_anchors\", num_neg_anchors / num_images)\n",
    "\n",
    "        localization_loss = _dense_box_regression_loss(\n",
    "            anchors,\n",
    "            self.box2box_transform,\n",
    "            pred_anchor_deltas,\n",
    "            gt_boxes,\n",
    "            pos_mask,\n",
    "            box_reg_loss_type=self.box_reg_loss_type,\n",
    "            smooth_l1_beta=self.smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        valid_mask = gt_labels >= 0\n",
    "        objectness_loss = F.binary_cross_entropy_with_logits(\n",
    "            cat(pred_objectness_logits, dim=1)[valid_mask],\n",
    "            gt_labels[valid_mask].to(torch.float32),\n",
    "            reduction=\"sum\",\n",
    "        )\n",
    "        normalizer = self.batch_size_per_image * num_images\n",
    "        losses = {\n",
    "            \"loss_rpn_cls\": objectness_loss / normalizer,\n",
    "            # The original Faster R-CNN paper uses a slightly different normalizer\n",
    "            # for loc loss. But it doesn't matter in practice\n",
    "            \"loss_rpn_loc\": localization_loss / normalizer,\n",
    "        }\n",
    "        losses = {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n",
    "        return losses\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: ImageList,\n",
    "        features: Dict[str, torch.Tensor],\n",
    "        gt_instances: Optional[List[Instances]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (ImageList): input images of length `N`\n",
    "            features (dict[str, Tensor]): input data as a mapping from feature\n",
    "                map name to tensor. Axis 0 represents the number of images `N` in\n",
    "                the input data; axes 1-3 are channels, height, and width, which may\n",
    "                vary between feature maps (e.g., if a feature pyramid is used).\n",
    "            gt_instances (list[Instances], optional): a length `N` list of `Instances`s.\n",
    "                Each `Instances` stores ground-truth instances for the corresponding image.\n",
    "\n",
    "        Returns:\n",
    "            proposals: list[Instances]: contains fields \"proposal_boxes\", \"objectness_logits\"\n",
    "            loss: dict[Tensor] or None\n",
    "        \"\"\"\n",
    "        features = [features[f] for f in self.in_features]\n",
    "        anchors = self.anchor_generator(features)\n",
    "\n",
    "        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)\n",
    "        # Transpose the Hi*Wi*A dimension to the middle:\n",
    "        pred_objectness_logits = [\n",
    "            # (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\n",
    "            score.permute(0, 2, 3, 1).flatten(1)\n",
    "            for score in pred_objectness_logits\n",
    "        ]\n",
    "        pred_anchor_deltas = [\n",
    "            # (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B) -> (N, Hi*Wi*A, B)\n",
    "            x.view(x.shape[0], -1, self.anchor_generator.box_dim, x.shape[-2], x.shape[-1])\n",
    "            .permute(0, 3, 4, 1, 2)\n",
    "            .flatten(1, -2)\n",
    "            for x in pred_anchor_deltas\n",
    "        ]\n",
    "\n",
    "        if self.training:\n",
    "            assert gt_instances is not None, \"RPN requires gt_instances in training!\"\n",
    "            gt_labels, gt_boxes = self.label_and_sample_anchors(anchors, gt_instances)\n",
    "            losses = self.losses(\n",
    "                anchors, pred_objectness_logits, gt_labels, pred_anchor_deltas, gt_boxes\n",
    "            )\n",
    "        else:\n",
    "            losses = {}\n",
    "        proposals = self.predict_proposals(\n",
    "            anchors, pred_objectness_logits, pred_anchor_deltas, images.image_sizes\n",
    "        )\n",
    "        return proposals, losses\n",
    "\n",
    "    def predict_proposals(\n",
    "        self,\n",
    "        anchors: List[Boxes],\n",
    "        pred_objectness_logits: List[torch.Tensor],\n",
    "        pred_anchor_deltas: List[torch.Tensor],\n",
    "        image_sizes: List[Tuple[int, int]],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Decode all the predicted box regression deltas to proposals. Find the top proposals\n",
    "        by applying NMS and removing boxes that are too small.\n",
    "\n",
    "        Returns:\n",
    "            proposals (list[Instances]): list of N Instances. The i-th Instances\n",
    "                stores post_nms_topk object proposals for image i, sorted by their\n",
    "                objectness score in descending order.\n",
    "        \"\"\"\n",
    "        # The proposals are treated as fixed for joint training with roi heads.\n",
    "        # This approach ignores the derivative w.r.t. the proposal boxesâ€™ coordinates that\n",
    "        # are also network responses.\n",
    "        with torch.no_grad():\n",
    "            pred_proposals = self._decode_proposals(anchors, pred_anchor_deltas)\n",
    "            return find_top_rpn_proposals(\n",
    "                pred_proposals,\n",
    "                pred_objectness_logits,\n",
    "                image_sizes,\n",
    "                self.nms_thresh,\n",
    "                self.pre_nms_topk[self.training],\n",
    "                self.post_nms_topk[self.training],\n",
    "                self.min_box_size,\n",
    "                self.training,\n",
    "            )\n",
    "\n",
    "    def _decode_proposals(self, anchors: List[Boxes], pred_anchor_deltas: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Transform anchors into proposals by applying the predicted anchor deltas.\n",
    "        Returns:\n",
    "            proposals (list[Tensor]): A list of L tensors. Tensor i has shape\n",
    "                (N, Hi*Wi*A, B)\n",
    "        \"\"\"\n",
    "        N = pred_anchor_deltas[0].shape[0]\n",
    "        proposals = []\n",
    "        # For each feature map\n",
    "        for anchors_i, pred_anchor_deltas_i in zip(anchors, pred_anchor_deltas):\n",
    "            B = anchors_i.tensor.size(1)\n",
    "            pred_anchor_deltas_i = pred_anchor_deltas_i.reshape(-1, B)\n",
    "            # Expand anchors to shape (N*Hi*Wi*A, B)\n",
    "            anchors_i = anchors_i.tensor.unsqueeze(0).expand(N, -1, -1).reshape(-1, B)\n",
    "            proposals_i = self.box2box_transform.apply_deltas(pred_anchor_deltas_i, anchors_i)\n",
    "            # Append feature map proposals with shape (N, Hi*Wi*A, B)\n",
    "            proposals.append(proposals_i.view(N, -1, B))\n",
    "        return proposals\n",
    "\n",
    "def build_proposal_generator_custom(cfg, input_shape):\n",
    "    return RPN(cfg, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A meta architecture that only predicts object proposals.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg, \n",
    "        backbone,\n",
    "        proposal_generator =\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone: a backbone module, must follow detectron2's backbone interface\n",
    "            proposal_generator: a module that generates proposals using backbone features\n",
    "            pixel_mean, pixel_std: list or tuple with #channels element, representing\n",
    "                the per-channel mean and std to be used to normalize the input image\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proposal_generator = proposal_generator\n",
    "        self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1), False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        backbone = build_backbone(cfg)\n",
    "        return {\n",
    "            \"backbone\": backbone,\n",
    "            \"proposal_generator\": build_proposal_generator(cfg, backbone.output_shape()),\n",
    "            \"pixel_mean\": cfg.MODEL.PIXEL_MEAN,\n",
    "            \"pixel_std\": cfg.MODEL.PIXEL_STD,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.pixel_mean.device\n",
    "\n",
    "    def _move_to_current_device(self, x):\n",
    "        return move_device_like(x, self.pixel_mean)\n",
    "\n",
    "    def forward(self, batched_inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Same as in :class:`GeneralizedRCNN.forward`\n",
    "\n",
    "        Returns:\n",
    "            list[dict]:\n",
    "                Each dict is the output for one input image.\n",
    "                The dict contains one key \"proposals\" whose value is a\n",
    "                :class:`Instances` with keys \"proposal_boxes\" and \"objectness_logits\".\n",
    "        \"\"\"\n",
    "        images = [self._move_to_current_device(x[\"image\"]) for x in batched_inputs]\n",
    "        images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n",
    "        images = ImageList.from_tensors(\n",
    "            images,\n",
    "            self.backbone.size_divisibility,\n",
    "            padding_constraints=self.backbone.padding_constraints,\n",
    "        )\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        if \"instances\" in batched_inputs[0]:\n",
    "            gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "        elif \"targets\" in batched_inputs[0]:\n",
    "            log_first_n(\n",
    "                logging.WARN, \"'targets' in the model inputs is now renamed to 'instances'!\", n=10\n",
    "            )\n",
    "            gt_instances = [x[\"targets\"].to(self.device) for x in batched_inputs]\n",
    "        else:\n",
    "            gt_instances = None\n",
    "        proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)\n",
    "        # In training, the proposals are not useful at all but we generate them anyway.\n",
    "        # This makes RPN-only models about 5% slower.\n",
    "        if self.training:\n",
    "            return proposal_losses\n",
    "\n",
    "        processed_results = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            proposals, batched_inputs, images.image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results.append({\"proposals\": r})\n",
    "        return processed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd.function import Function\n",
    "\n",
    "from detectron2.config import configurable\n",
    "from detectron2.layers import ShapeSpec\n",
    "from detectron2.structures import Boxes, Instances, pairwise_iou\n",
    "from detectron2.utils.events import get_event_storage\n",
    "\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.matcher import Matcher\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from detectron2.modeling.roi_heads.box_head import build_box_head\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, fast_rcnn_inference\n",
    "from detectron2.modeling.roi_heads.roi_heads import ROI_HEADS_REGISTRY, StandardROIHeads\n",
    "\n",
    "\n",
    "class _ScaleGradient(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, scale):\n",
    "        ctx.scale = scale\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * ctx.scale, None\n",
    "\n",
    "\n",
    "class CascadeROIHeads(StandardROIHeads):\n",
    "    \"\"\"\n",
    "    The ROI heads that implement :paper:`Cascade R-CNN`.\n",
    "    \"\"\"\n",
    "\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        box_in_features: List[str],\n",
    "        box_pooler: ROIPooler,\n",
    "        box_heads: List[nn.Module],\n",
    "        box_predictors: List[nn.Module],\n",
    "        proposal_matchers: List[Matcher],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        NOTE: this interface is experimental.\n",
    "\n",
    "        Args:\n",
    "            box_pooler (ROIPooler): pooler that extracts region features from given boxes\n",
    "            box_heads (list[nn.Module]): box head for each cascade stage\n",
    "            box_predictors (list[nn.Module]): box predictor for each cascade stage\n",
    "            proposal_matchers (list[Matcher]): matcher with different IoU thresholds to\n",
    "                match boxes with ground truth for each stage. The first matcher matches\n",
    "                RPN proposals with ground truth, the other matchers use boxes predicted\n",
    "                by the previous stage as proposals and match them with ground truth.\n",
    "        \"\"\"\n",
    "        assert \"proposal_matcher\" not in kwargs, (\n",
    "            \"CascadeROIHeads takes 'proposal_matchers=' for each stage instead \"\n",
    "            \"of one 'proposal_matcher='.\"\n",
    "        )\n",
    "        # The first matcher matches RPN proposals with ground truth, done in the base class\n",
    "        kwargs[\"proposal_matcher\"] = proposal_matchers[0]\n",
    "        num_stages = self.num_cascade_stages = len(box_heads)\n",
    "        box_heads = nn.ModuleList(box_heads)\n",
    "        box_predictors = nn.ModuleList(box_predictors)\n",
    "        assert len(box_predictors) == num_stages, f\"{len(box_predictors)} != {num_stages}!\"\n",
    "        assert len(proposal_matchers) == num_stages, f\"{len(proposal_matchers)} != {num_stages}!\"\n",
    "        super().__init__(\n",
    "            box_in_features=box_in_features,\n",
    "            box_pooler=box_pooler,\n",
    "            box_head=box_heads,\n",
    "            box_predictor=box_predictors,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.proposal_matchers = proposal_matchers\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, input_shape):\n",
    "        ret = super().from_config(cfg, input_shape)\n",
    "        ret.pop(\"proposal_matcher\")\n",
    "        return ret\n",
    "\n",
    "    @classmethod\n",
    "    def _init_box_head(cls, cfg, input_shape):\n",
    "        # fmt: off\n",
    "        in_features              = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
    "        pooler_resolution        = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n",
    "        pooler_scales            = tuple(1.0 / input_shape[k].stride for k in in_features)\n",
    "        sampling_ratio           = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n",
    "        pooler_type              = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE\n",
    "        cascade_bbox_reg_weights = cfg.MODEL.ROI_BOX_CASCADE_HEAD.BBOX_REG_WEIGHTS\n",
    "        cascade_ious             = cfg.MODEL.ROI_BOX_CASCADE_HEAD.IOUS\n",
    "        assert len(cascade_bbox_reg_weights) == len(cascade_ious)\n",
    "        assert cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG,  \\\n",
    "            \"CascadeROIHeads only support class-agnostic regression now!\"\n",
    "        assert cascade_ious[0] == cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS[0]\n",
    "        # fmt: on\n",
    "\n",
    "        in_channels = [input_shape[f].channels for f in in_features]\n",
    "        # Check all channel counts are equal\n",
    "        assert len(set(in_channels)) == 1, in_channels\n",
    "        in_channels = in_channels[0]\n",
    "\n",
    "        box_pooler = ROIPooler(\n",
    "            output_size=pooler_resolution,\n",
    "            scales=pooler_scales,\n",
    "            sampling_ratio=sampling_ratio,\n",
    "            pooler_type=pooler_type,\n",
    "        )\n",
    "        pooled_shape = ShapeSpec(\n",
    "            channels=in_channels, width=pooler_resolution, height=pooler_resolution\n",
    "        )\n",
    "\n",
    "        box_heads, box_predictors, proposal_matchers = [], [], []\n",
    "        for match_iou, bbox_reg_weights in zip(cascade_ious, cascade_bbox_reg_weights):\n",
    "            box_head = build_box_head(cfg, pooled_shape)\n",
    "            box_heads.append(box_head)\n",
    "            box_predictors.append(\n",
    "                FastRCNNOutputLayers(\n",
    "                    cfg,\n",
    "                    box_head.output_shape,\n",
    "                    box2box_transform=Box2BoxTransform(weights=bbox_reg_weights),\n",
    "                )\n",
    "            )\n",
    "            proposal_matchers.append(Matcher([match_iou], [0, 1], allow_low_quality_matches=False))\n",
    "        return {\n",
    "            \"box_in_features\": in_features,\n",
    "            \"box_pooler\": box_pooler,\n",
    "            \"box_heads\": box_heads,\n",
    "            \"box_predictors\": box_predictors,\n",
    "            \"proposal_matchers\": proposal_matchers,\n",
    "        }\n",
    "\n",
    "    def forward(self, images, features, proposals, targets=None):\n",
    "        del images\n",
    "        if self.training:\n",
    "            proposals = self.label_and_sample_proposals(proposals, targets)\n",
    "\n",
    "        if self.training:\n",
    "            # Need targets to box head\n",
    "            losses = self._forward_box(features, proposals, targets)\n",
    "            losses.update(self._forward_mask(features, proposals))\n",
    "            losses.update(self._forward_keypoint(features, proposals))\n",
    "            return proposals, losses\n",
    "        else:\n",
    "            pred_instances = self._forward_box(features, proposals)\n",
    "            pred_instances = self.forward_with_given_boxes(features, pred_instances)\n",
    "            return pred_instances, {}\n",
    "\n",
    "    def _forward_box(self, features, proposals, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features, targets: the same as in\n",
    "                Same as in :meth:`ROIHeads.forward`.\n",
    "            proposals (list[Instances]): the per-image object proposals with\n",
    "                their matching ground truth.\n",
    "                Each has fields \"proposal_boxes\", and \"objectness_logits\",\n",
    "                \"gt_classes\", \"gt_boxes\".\n",
    "        \"\"\"\n",
    "        features = [features[f] for f in self.box_in_features]\n",
    "        head_outputs = []  # (predictor, predictions, proposals)\n",
    "        prev_pred_boxes = None\n",
    "        image_sizes = [x.image_size for x in proposals]\n",
    "        for k in range(self.num_cascade_stages):\n",
    "            if k > 0:\n",
    "                # The output boxes of the previous stage are used to create the input\n",
    "                # proposals of the next stage.\n",
    "                proposals = self._create_proposals_from_boxes(prev_pred_boxes, image_sizes)\n",
    "                if self.training:\n",
    "                    proposals = self._match_and_label_boxes(proposals, k, targets)\n",
    "            predictions = self._run_stage(features, proposals, k)\n",
    "            prev_pred_boxes = self.box_predictor[k].predict_boxes(predictions, proposals)\n",
    "            head_outputs.append((self.box_predictor[k], predictions, proposals))\n",
    "\n",
    "        if self.training:\n",
    "            losses = {}\n",
    "            storage = get_event_storage()\n",
    "            for stage, (predictor, predictions, proposals) in enumerate(head_outputs):\n",
    "                with storage.name_scope(\"stage{}\".format(stage)):\n",
    "                    stage_losses = predictor.losses(predictions, proposals)\n",
    "                losses.update({k + \"_stage{}\".format(stage): v for k, v in stage_losses.items()})\n",
    "            return losses\n",
    "        else:\n",
    "            # Each is a list[Tensor] of length #image. Each tensor is Ri x (K+1)\n",
    "            scores_per_stage = [h[0].predict_probs(h[1], h[2]) for h in head_outputs]\n",
    "\n",
    "            # Average the scores across heads\n",
    "            scores = [\n",
    "                sum(list(scores_per_image)) * (1.0 / self.num_cascade_stages)\n",
    "                for scores_per_image in zip(*scores_per_stage)\n",
    "            ]\n",
    "            # Use the boxes of the last head\n",
    "            predictor, predictions, proposals = head_outputs[-1]\n",
    "            boxes = predictor.predict_boxes(predictions, proposals)\n",
    "            pred_instances, _ = fast_rcnn_inference(\n",
    "                boxes,\n",
    "                scores,\n",
    "                image_sizes,\n",
    "                predictor.test_score_thresh,\n",
    "                predictor.test_nms_thresh,\n",
    "                predictor.test_topk_per_image,\n",
    "            )\n",
    "            return pred_instances\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _match_and_label_boxes(self, proposals, stage, targets):\n",
    "        \"\"\"\n",
    "        Match proposals with groundtruth using the matcher at the given stage.\n",
    "        Label the proposals as foreground or background based on the match.\n",
    "\n",
    "        Args:\n",
    "            proposals (list[Instances]): One Instances for each image, with\n",
    "                the field \"proposal_boxes\".\n",
    "            stage (int): the current stage\n",
    "            targets (list[Instances]): the ground truth instances\n",
    "\n",
    "        Returns:\n",
    "            list[Instances]: the same proposals, but with fields \"gt_classes\" and \"gt_boxes\"\n",
    "        \"\"\"\n",
    "        num_fg_samples, num_bg_samples = [], []\n",
    "        for proposals_per_image, targets_per_image in zip(proposals, targets):\n",
    "            match_quality_matrix = pairwise_iou(\n",
    "                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes\n",
    "            )\n",
    "            # proposal_labels are 0 or 1\n",
    "            matched_idxs, proposal_labels = self.proposal_matchers[stage](match_quality_matrix)\n",
    "            if len(targets_per_image) > 0:\n",
    "                gt_classes = targets_per_image.gt_classes[matched_idxs]\n",
    "                # Label unmatched proposals (0 label from matcher) as background (label=num_classes)\n",
    "                gt_classes[proposal_labels == 0] = self.num_classes\n",
    "                gt_boxes = targets_per_image.gt_boxes[matched_idxs]\n",
    "            else:\n",
    "                gt_classes = torch.zeros_like(matched_idxs) + self.num_classes\n",
    "                gt_boxes = Boxes(\n",
    "                    targets_per_image.gt_boxes.tensor.new_zeros((len(proposals_per_image), 4))\n",
    "                )\n",
    "            proposals_per_image.gt_classes = gt_classes\n",
    "            proposals_per_image.gt_boxes = gt_boxes\n",
    "\n",
    "            num_fg_samples.append((proposal_labels == 1).sum().item())\n",
    "            num_bg_samples.append(proposal_labels.numel() - num_fg_samples[-1])\n",
    "\n",
    "        # Log the number of fg/bg samples in each stage\n",
    "        storage = get_event_storage()\n",
    "        storage.put_scalar(\n",
    "            \"stage{}/roi_head/num_fg_samples\".format(stage),\n",
    "            sum(num_fg_samples) / len(num_fg_samples),\n",
    "        )\n",
    "        storage.put_scalar(\n",
    "            \"stage{}/roi_head/num_bg_samples\".format(stage),\n",
    "            sum(num_bg_samples) / len(num_bg_samples),\n",
    "        )\n",
    "        return proposals\n",
    "\n",
    "    def _run_stage(self, features, proposals, stage):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (list[Tensor]): #lvl input features to ROIHeads\n",
    "            proposals (list[Instances]): #image Instances, with the field \"proposal_boxes\"\n",
    "            stage (int): the current stage\n",
    "\n",
    "        Returns:\n",
    "            Same output as `FastRCNNOutputLayers.forward()`.\n",
    "        \"\"\"\n",
    "        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
    "        # The original implementation averages the losses among heads,\n",
    "        # but scale up the parameter gradients of the heads.\n",
    "        # This is equivalent to adding the losses among heads,\n",
    "        # but scale down the gradients on features.\n",
    "        if self.training:\n",
    "            box_features = _ScaleGradient.apply(box_features, 1.0 / self.num_cascade_stages)\n",
    "        box_features = self.box_head[stage](box_features)\n",
    "        return self.box_predictor[stage](box_features)\n",
    "\n",
    "    def _create_proposals_from_boxes(self, boxes, image_sizes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            boxes (list[Tensor]): per-image predicted boxes, each of shape Ri x 4\n",
    "            image_sizes (list[tuple]): list of image shapes in (h, w)\n",
    "        Returns:\n",
    "            list[Instances]: per-image proposals with the given boxes.\n",
    "        \"\"\"\n",
    "        # Just like RPN, the proposals should not have gradients\n",
    "        boxes = [Boxes(b.detach()) for b in boxes]\n",
    "        proposals = []\n",
    "        for boxes_per_image, image_size in zip(boxes, image_sizes):\n",
    "            boxes_per_image.clip(image_size)\n",
    "            if self.training:\n",
    "                # do not filter empty boxes at inference time,\n",
    "                # because the scores from each stage need to be aligned and added later\n",
    "                boxes_per_image = boxes_per_image[boxes_per_image.nonempty()]\n",
    "            prop = Instances(image_size)\n",
    "            prop.proposal_boxes = boxes_per_image\n",
    "            proposals.append(prop)\n",
    "        return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the whole architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai.ditod.VGTbackbone import build_VGT_fpn_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from detectron2.config import get_cfg\n",
    "from config import get_settings\n",
    "from ai.ditod import add_vit_config\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    dataset = \"doclaynet\"\n",
    "    config_file_path = \"ai/Configs/doclaynet_VGT_cascade_PTM.yaml\"\n",
    "    model_weights_path = \"./models/model_final.pth\"\n",
    "\n",
    "\n",
    "args = Arguments()\n",
    "cfg = get_cfg()\n",
    "add_vit_config(cfg)\n",
    "cfg.merge_from_file(args.config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = build_VGT_fpn_backbone(cfg, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([ShapeSpec(channels=256, height=None, width=None, stride=4), ShapeSpec(channels=256, height=None, width=None, stride=8), ShapeSpec(channels=256, height=None, width=None, stride=16), ShapeSpec(channels=256, height=None, width=None, stride=32), ShapeSpec(channels=256, height=None, width=None, stride=64)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.output_shape().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_generator = DefaultAnchorGenerator(cfg, backbone.output_shape().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 /n 256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RPN(\n",
       "  (rpn_head): StandardRPNHead(\n",
       "    (conv): Conv2d(\n",
       "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (anchor_generator): DefaultAnchorGenerator(\n",
       "    (cell_anchors): BufferList()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_proposal_generator_custom(cfg, backbone.output_shape().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VGT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
